{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nea9bRETPmao"
      },
      "source": [
        "# 1. Setup Instructions\n",
        "\n",
        "Before executin the code, follow these steps for a smooth experience:\n",
        "\n",
        "## I. Initial Setup and Preparation\n",
        "- ### Selecting the Hardware Accelerator\n",
        "  - Navigate to \"Runtime\" -> \"Change runtime type\" in the Colab menu.\n",
        "  - Choose \"GPU\" as your hardware accelerator for optimal performance.\n",
        "\n",
        "- ### Python Version Check\n",
        "  - Execute `!python --version` in a new cell to confirm your Python version.\n",
        "  - This notebook is optimized for Python 3.10.\n",
        "\n",
        "- ### Accessing Necessary Files\n",
        "  - All required files, including datasets and embeddings, are in our GitHub repository.\n",
        "  - Download these files and upload them to your Google Drive in a structured directory, like \"/content/drive/MyDrive/project/PalliativEtiological\n",
        "/\".\n",
        "\n",
        "- ### Google Drive / Colab's local disk\n",
        "  - Mount your Google Drive to access the files.\n",
        "  - Alternatively, upload files to Colab's local disk.\n",
        "\n",
        "- ### Run Code\n",
        "  To run specific sections or the entire code efficiently, follow these guidelines:\n",
        "  \n",
        "  - **Initial Setup:**\n",
        "    - Begin with the setup instructions to establish the necessary environment.\n",
        "    - Make sure the paths for all required files are correctly set.\n",
        "\n",
        "  - **Running Specific Sections (Sections 2-9):**\n",
        "    - **File Dependencies:**\n",
        "      - Check each section's instructions for its file requirements.\n",
        "      - Ensure these files are loaded and their paths are correctly set.\n",
        "    - **Independence of Sections:**\n",
        "      - Sections can run independently, **except for section 7**, which needs the 10-fold CV SciBert. Confirm that all necessary files for each section are available.\n",
        "    - **Note:**\n",
        "      - Before running a specific section, it is essential to verify that all classes, methods, and files required by that section are correctly loaded and executed.\n",
        "\n",
        "\n",
        "  - **Running the Entire Notebook:**\n",
        "    - Go to \"Runtime\" in the Colab menu and select \"Run all\".\n",
        "    - **Key File Check:** Ensure `comprehensive_dataset.csv.gz` is available and loaded, as it's critical for the entire notebook.\n",
        "\n",
        "  - **Source of Files:**\n",
        "    - Download necessary files from our GitHub repository.\n",
        "    - Store them correctly in your Google Drive or Colab's local disk.\n",
        "\n",
        "\n",
        "\n",
        "## **II. Using Pre-generated Resources**\n",
        "- ### Pre-generated Embeddings\n",
        "  - Generating embeddings for the GNN model can be quite time-consuming.\n",
        "  - To expedite the process, you may choose to use pre-generated embeddings from our GitHub repository.\n",
        "  - Instructions for using these embeddings are provided in the relevant sections of this notebook.\n",
        "\n",
        "## **III. Fine-Tuned SciBERT Model**\n",
        "  - Please note: To initialize the tokenizer for the SciBERT model, the use of an access token is optional. While an access token can be used (obtainable from your Hugging Face account), it's not mandatory for accessing public models like SciBERT. If you have an access token, assign it to the variable access_token. Otherwise, set the variable to None.\n",
        "\n",
        "## **IV. Comparing GNN Model and SciBERT**\n",
        "- ### Fine-Tuning for Comparison\n",
        "  - To effectively compare the performance between the GNN model and SciBERT, it's essential to fine-tune both using the same 10-fold cross-validation process.\n",
        "  - Detailed instructions for fine-tuning SciBERT using 10-fold cross-validation are provided in the respective sections of this notebook.\n",
        "\n",
        "## **V. Note on Naming Conventions in Code**:\n",
        "- In the development of our machine learning model, which we refer to as **DruGNNosis-MoA** in the paper, <br>\n",
        "the code utilizes a more concise variable name and associated class name, **hetroSciGNN**. This decision was made to streamline the coding process and enhance readability during development."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcAv_6fNQIWg"
      },
      "outputs": [],
      "source": [
        "# check python version\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v7Pw13Cx7gN"
      },
      "source": [
        "## Mounting Google Drive and Specifying Paths for Dataset, Model, and Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYgbZINt0ws3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True) # Mounting Google Drive for File Access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFJ5Piuk01kc"
      },
      "source": [
        "Specify filenames for the embeddings generated using SciBERT\n",
        "Define the file paths for embeddings to be used as input in the GNN model.\n",
        "Adjust the paths as per your directory structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0SHc1Kea6yD"
      },
      "outputs": [],
      "source": [
        "comprehensive_dataset_path = '/content/drive/MyDrive/project/GNN/comprehensive_dataset.csv.gz'  # Replace with your dataset path before running\n",
        "\n",
        "cv_index_file_path = 'fold_indexes.pkl'\n",
        "\n",
        "\"\"\"\n",
        "Fine tuned SciBERT model\n",
        "\"\"\"\n",
        "# Define the path of the directory to be compressed\n",
        "model_dir_format = 'fine_tuned_scibert_fold_{}' # Base directory name\n",
        "compressed_model_format = 'fine_tuned_scibert_fold_{}.tar.gz' # Format for compressed model name\n",
        "\n",
        "\"\"\"\n",
        "Initialize the tokenizer for SciBERT. The 'token' argument is optional.\n",
        "If you have an access token from Hugging Face (which can be generated from your Hugging Face account),\n",
        "you can use it here by assigning it to 'access_token'.\n",
        "However, for accessing public models like the one we use (SciBERT), it's optional.\n",
        "\"\"\"\n",
        "access_token = None # Replace with your Hugging Face access token, or set to None if you don't have one.\n",
        "\n",
        "\"\"\"\n",
        "Embeddings\n",
        "\"\"\"\n",
        "text_embeddings_file_path_format = 'text_embeddings_fold_{}.pkl'\n",
        "eigen_featues_path = 'eigen_features_baseModel_combined.pt'\n",
        "# Define the original and reduced embedding dimensions\n",
        "original_embedding_dim = 768 # Do not change\n",
        "reduced_embedding_dim = 128 # Do not change\n",
        "\n",
        "\"\"\"\n",
        "Paths for results metrices\n",
        "\"\"\"\n",
        "SciBERT_valid_fold_metrics_path = 'SciBERT_valid_fold_metrics.pkl'\n",
        "hetroSciGNN_cv_results_path = 'DruGNNosis-MoA_cv_results.pkl'\n",
        "baseGNN_cv_results_path = 'baseGNN_cv_results.pkl'\n",
        "hetroSciGNN_test_metrics_path = 'DruGNNosis-MoA_test_metrics.pkl'\n",
        "\n",
        "\"\"\"\n",
        "Best model\n",
        "\"\"\"\n",
        "fine_tuned_90DataSet_path = 'fine_tuned_scibert_noValidSet'\n",
        "compressed_bestModel_format = 'fine_tuned_scibert_noValidSet.tar.gz'\n",
        "\n",
        "# The path for the text embeddings for the best model\n",
        "best_model_gene_features_path =  'gene_embeddings_tensor_final.pt'\n",
        "best_model_disease_features_path = 'disease_embeddings_tensor_final.pt'\n",
        "best_model_drug_features_path = 'drug_embeddings_tensor_final.pt'\n",
        "\n",
        "best_model_path = 'DruGNNosis-MoA_model.pth'\n",
        "best_inputData_path = 'DruGNNosis-MoA_data.pkl'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKjZX8bVAueB"
      },
      "source": [
        "## Installing Necessary Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRK_Ko7uXQyy"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k50Ef24yD9uS"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbRew-hjipN8"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCKOQmv-iqdg"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6_RUEQ3BAAi"
      },
      "source": [
        "## Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytrUr_lbUCN1"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import csv\n",
        "import tarfile\n",
        "import gc\n",
        "import random\n",
        "import pickle\n",
        "import gzip\n",
        "import shutil\n",
        "from itertools import product\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Scientific computing and data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from scipy import stats\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.linalg import eigs\n",
        "\n",
        "# Machine Learning and Deep Learning frameworks\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Optimization algorithms from PyTorch\n",
        "from torch.optim import Adam, SGD, AdamW, LBFGS, Adadelta, RMSprop\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "# PyTorch Geometric (PyG) for Graph Neural Networks (GNNs)\n",
        "import torch_geometric\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from torch_geometric.nn import (GraphConv, GATConv, GCNConv, SAGEConv, GATv2Conv, Linear, HeteroConv, HGTConv, RGCNConv, RGATConv, MessagePassing, global_add_pool, Node2Vec)\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.explain import GNNExplainer\n",
        "# scikit-learn\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report, roc_auc_score, auc, accuracy_score, balanced_accuracy_score, precision_score, recall_score, roc_curve\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Plotting and visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.lines import Line2D\n",
        "import networkx as nx\n",
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "\n",
        "# NLP and transformers\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, AutoModel\n",
        "from datasets import Dataset, DatasetDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyFpkyTY2DVP"
      },
      "source": [
        "Generate requirements.txt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdCR5IAf0shY"
      },
      "outputs": [],
      "source": [
        "import pkg_resources\n",
        "import platform\n",
        "\n",
        "# List of third-party libraries\n",
        "libraries = [\n",
        "    'numpy', 'pandas', 'scipy', 'torch', 'torch-geometric',\n",
        "    'scikit-learn', 'seaborn', 'matplotlib', 'networkx',\n",
        "    'transformers', 'datasets'\n",
        "]\n",
        "\n",
        "# Get the current Python version\n",
        "python_version = platform.python_version()\n",
        "\n",
        "# File to write the requirements\n",
        "with open('requirements.txt', 'w') as f:\n",
        "\n",
        "    f.write(f\"# Python version: {python_version}\\n\")\n",
        "\n",
        "    for lib in libraries:\n",
        "        try:\n",
        "            version = pkg_resources.get_distribution(lib).version\n",
        "            f.write(f\"{lib}=={version}\\n\")\n",
        "        except pkg_resources.DistributionNotFound:\n",
        "            f.write(f\"# {lib} not found\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM691hP7xtf4"
      },
      "source": [
        " ## Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF30Zp5dxmwR"
      },
      "outputs": [],
      "source": [
        "# Load data sets that contain all the information we need for the graph.\n",
        "df_all = pd.read_csv(comprehensive_dataset_path) # Check that the path for the dataset is correct\n",
        "\n",
        "# Filter df that contain only drugs with e/p.\n",
        "df = df_all[df_all['MoA'] != 'both']\n",
        "\n",
        "# Reset the index of the DataFrame to ensure continuity after filtering rows.\n",
        "df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmlZRQNgBRIW"
      },
      "source": [
        "# 2. Preprocess The Dataset & Construct a Heterogeneous Network Using PyG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1v9fQj_CTiw"
      },
      "source": [
        "Setting Seed for Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH5DSlAyMNBA"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed_value=24):\n",
        "    \"\"\"Set seed for reproducibility.\"\"\"\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(24)  # Initialize the seed with a specific value (24) for reproducibility.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzWMSQ61XjET"
      },
      "source": [
        "Constructing a Heterogeneous Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_hASe33XOBr"
      },
      "outputs": [],
      "source": [
        "# 1. Create node mappings\n",
        "\n",
        "# Drugs\n",
        "unique_drugs = df.loc[df['Source Type'] == 'Drug', 'Source Name'].unique().tolist()\n",
        "drug_mapping = {drug: i for i, drug in enumerate(unique_drugs)}\n",
        "\n",
        "# Diseases\n",
        "unique_diseases = df[df['Source Type'] == 'Disease']['Source Name'].unique().tolist()\n",
        "disease_mapping = {disease: i for i, disease in enumerate(unique_diseases)}\n",
        "\n",
        "# Genes (Include all genes)\n",
        "genes_from_source = df[df['Source Type'] == 'Gene']['Source Name'].unique().tolist()\n",
        "genes_from_target = df[df['Target Type'] == 'Gene']['Target Name'].unique().tolist()\n",
        "all_genes = list(set(genes_from_source + genes_from_target))\n",
        "gene_mapping = {gene: i for i, gene in enumerate(all_genes)}\n",
        "\n",
        "# 2. Process node labels for drugs\n",
        "\n",
        "# Extract unique drug information\n",
        "drug_info_df = df[df['Source Type'] == 'Drug'].drop_duplicates(subset=['Source Name'])\n",
        "\n",
        "# Convert MoA to labels\n",
        "drug_info_df['MoA'] = drug_info_df['MoA'].apply(lambda x: 0 if x == 'p' else (1 if x == 'e' else x))\n",
        "\n",
        "# Convert the 'MoA' column to a numpy array of type float32, then to a PyTorch tensor\n",
        "drug_y = torch.tensor(drug_info_df['MoA'].values.astype(np.float32))\n",
        "\n",
        "# Create a mask to identify which drugs have labels (MoA not equal to -1, can be usefull in the feature.)\n",
        "drug_mask = drug_y != -1\n",
        "\n",
        "# 3. Create edge indices for the various interactions\n",
        "\n",
        "# Drug-Gene interactions\n",
        "drug_gene_edges_df = df[(df['Source Type'] == 'Drug') & (df['Target Type'] == 'Gene')]\n",
        "source_nodes_dg = drug_gene_edges_df[\"Source Name\"].map(drug_mapping).dropna().astype(int).values\n",
        "target_nodes_dg = drug_gene_edges_df['Target Name'].map(gene_mapping).dropna().astype(int).values\n",
        "edge_index_dg = torch.tensor(np.array([source_nodes_dg, target_nodes_dg]), dtype=torch.long)\n",
        "\n",
        "# Disease-Gene interactions\n",
        "disease_gene_edges_df = df[(df['Source Type'] == 'Disease') & (df['Target Type'] == 'Gene')]\n",
        "source_nodes_pg = disease_gene_edges_df['Source Name'].map(disease_mapping).dropna().astype(int).values\n",
        "target_nodes_pg = disease_gene_edges_df['Target Name'].map(gene_mapping).dropna().astype(int).values\n",
        "edge_index_pg = torch.tensor(np.array([source_nodes_pg, target_nodes_pg]), dtype=torch.long)\n",
        "\n",
        "# Gene-Gene interactions (PPI)\n",
        "gene_gene_edges_df = df[(df['Source Type'] == 'Gene') & (df['Target Type'] == 'Gene')]\n",
        "source_nodes_gg = gene_gene_edges_df['Source Name'].map(gene_mapping).dropna().astype(int).values\n",
        "target_nodes_gg = gene_gene_edges_df['Target Name'].map(gene_mapping).dropna().astype(int).values\n",
        "edge_index_gg = torch.tensor(np.array([source_nodes_gg, target_nodes_gg]), dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ScoOKaqlSvc"
      },
      "source": [
        "Setting Up Train-Validation and Test Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyAbtRi0lJRX"
      },
      "outputs": [],
      "source": [
        "# Calculate total number of nodes\n",
        "num_nodes_drugs = len(unique_drugs)\n",
        "\n",
        "# Split the drugs into training/validation and test sets\n",
        "train_val_drugs, test_drugs = train_test_split(unique_drugs, test_size=0.1, random_state=24, stratify=drug_info_df['MoA'])\n",
        "\n",
        "# Get the indices for training/validation and test drugs\n",
        "train_val_indices = drug_info_df[drug_info_df['Source Name'].isin(train_val_drugs)].index\n",
        "test_indices = drug_info_df[drug_info_df['Source Name'].isin(test_drugs)].index\n",
        "\n",
        "train_val_labels = drug_info_df.loc[train_val_indices, 'MoA'].values\n",
        "test_labels = drug_info_df.loc[test_indices, 'MoA'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2amXN4gEvzGy"
      },
      "source": [
        "Setting up a Stratified K-Fold cross-validation process & saving the fold indices to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNyBTOkevycT"
      },
      "outputs": [],
      "source": [
        "# Initialize Stratified K-Fold for the cross-validation and store fold indexes\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=24)\n",
        "fold_indexes = [(train_idx, val_idx) for train_idx, val_idx in skf.split(train_val_indices, train_val_labels)]\n",
        "\n",
        "# Save fold_indexes to a file\n",
        "with open(cv_index_file_path, 'wb') as f:\n",
        "    pickle.dump(fold_indexes, f)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Determine whether a GPU is available for computation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLrqMxjKAN3E"
      },
      "source": [
        "# 3. SciBERT Model Classes & Methods\n",
        "For more information on SciBERT, visit: https://github.com/allenai/scibert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i72EOtH-740d"
      },
      "source": [
        "### Defining Training Data, Tokenizer, Methods for Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca8TcF2XARui"
      },
      "outputs": [],
      "source": [
        "def extract_embeddings(model, tokenizer, sentences):\n",
        "    model.to(device)\n",
        "    tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "    tokens = tokens.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings.cpu()\n",
        "\n",
        "# Function to apply PCA and reduce dimensionality\n",
        "def apply_pca(embeddings, n_components):\n",
        "    pca = PCA(n_components=n_components)\n",
        "    return pca.fit_transform(embeddings)\n",
        "\n",
        "# Function to compute metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, predictions),\n",
        "        'f1': f1_score(labels, predictions),\n",
        "        'precision': precision_score(labels, predictions),\n",
        "        'recall': recall_score(labels, predictions),\n",
        "        'roc_auc': roc_auc_score(labels, logits[:,1])\n",
        "\n",
        "    }\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T2xAsxQAYyg"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store probabilities, true labels, and metrics\n",
        "all_true_labels = []\n",
        "all_pred_probs = []\n",
        "SciBERT_valid_fold_metrics = []\n",
        "\n",
        "# Extract the \"Drug_Description\"\n",
        "drug_info_df['text'] = drug_info_df['Drug_Description'].fillna('')\n",
        "\n",
        "train_val_df = drug_info_df.iloc[train_val_indices].copy() # Create the train_valid df with the same 'test_indices' we used for the GNN.\n",
        "\n",
        "train_val_df.rename(columns={'MoA': 'label'}, inplace=True)\n",
        "\n",
        "# Tokenization - Using an access token is optional for public models - include it with token=access_token if you have one, or omit it otherwise.\n",
        "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', token=access_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXkwliNcAf0m"
      },
      "source": [
        "### SciBERT Training and Embedding Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6zKAnaPAgL2"
      },
      "outputs": [],
      "source": [
        "# Load fold_indexes from the file\n",
        "with open(cv_index_file_path, 'rb') as f:\n",
        "    fold_indexes = pickle.load(f)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(fold_indexes):\n",
        "\n",
        "    print(f\"Training for fold {fold}\")\n",
        "\n",
        "    print(torch.cuda.is_available())\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Determine whether a GPU is available for computation.\n",
        "\n",
        "    # Splitting data into train and validation for current fold\n",
        "    train_df = train_val_df.iloc[train_idx]\n",
        "    val_df = train_val_df.iloc[val_idx]\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df).map(preprocess_function, batched=True)\n",
        "    val_dataset = Dataset.from_pandas(val_df).map(preprocess_function, batched=True)\n",
        "\n",
        "    # Model and Training\n",
        "    model_LLM = AutoModelForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels=2, token=access_token)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'.llm_model_output/llm_model_output_fold_{fold}',\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.002,\n",
        "        evaluation_strategy=\"epoch\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model_LLM,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    valid_results = trainer.evaluate(val_dataset)\n",
        "    SciBERT_valid_fold_metrics.append(valid_results)\n",
        "\n",
        "    # Get predictions for ROC curve analysis\n",
        "    predictions = trainer.predict(val_dataset)\n",
        "    logits = predictions.predictions\n",
        "    true_labels = predictions.label_ids\n",
        "    softmax = torch.nn.Softmax(dim=1)\n",
        "    pred_probs = softmax(torch.from_numpy(logits)).numpy()[:, 1]  # Assuming class 1 is the positive class\n",
        "    all_true_labels.extend(true_labels)\n",
        "    all_pred_probs.extend(pred_probs)\n",
        "\n",
        "\n",
        "    dir_fine_tuned_scibert_current_fold = model_dir_format.format(fold)\n",
        "    compressed_model_name = compressed_model_format.format(fold)\n",
        "\n",
        "\n",
        "    model_LLM.save_pretrained(dir_fine_tuned_scibert_current_fold)\n",
        "\n",
        "    # Compress the model directory\n",
        "    with tarfile.open(compressed_model_name, \"w:gz\") as tar:\n",
        "        tar.add(dir_fine_tuned_scibert_current_fold, arcname=os.path.basename(dir_fine_tuned_scibert_current_fold))\n",
        "\n",
        "    # Delete the uncompressed model directory\n",
        "    shutil.rmtree(dir_fine_tuned_scibert_current_fold)\n",
        "\n",
        "    # Clear the CUDA cache if using GPU\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Explicitly invoke garbage collection\n",
        "    gc.collect()\n",
        "\n",
        "# Save metrics and predictions for later analysis\n",
        "with open(SciBERT_valid_fold_metrics_path, 'wb') as file:\n",
        "    pickle.dump({\n",
        "        'metrics': SciBERT_valid_fold_metrics,\n",
        "        'true_labels': all_true_labels,\n",
        "        'pred_probs': all_pred_probs\n",
        "    }, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSrh5qcS5xBr"
      },
      "source": [
        "# 4. Graph Neural Network (Baseline & DruGNNosis-MoA models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni_uUYbi_MkF"
      },
      "source": [
        "## Methods: Generate new HetroData object, Training & Evalution for GNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYrr7Kdk5Jwq"
      },
      "outputs": [],
      "source": [
        "transform = T.Compose([T.ToUndirected()])\n",
        "\n",
        "# Function to rebuild the graph\n",
        "def rebuild_graph(df, drug_mapping, gene_mapping, disease_mapping, drug_features, gene_features, disease_features, drug_y):\n",
        "    data = HeteroData() # Create the HeteroData object will be used to store the heterogeneous graph\n",
        "\n",
        "    data['gene'].x = gene_features # Set gene features\n",
        "    data['disease'].x = disease_features # Set disease features\n",
        "    data['drug'].x = drug_features # Set drug features\n",
        "\n",
        "    data['gene'].num_nodes = len(gene_mapping)\n",
        "    data['drug'].num_nodes = len(drug_mapping)\n",
        "    data['disease'].num_nodes = len(disease_mapping)\n",
        "\n",
        "    data['drug'].y = drug_y # Set drug labels.\n",
        "    data['drug', 'drug_target', 'gene'].edge_index = edge_index_dg # Set drug-gene edges\n",
        "    data['disease', 'disease_gene', 'gene'].edge_index = edge_index_pg # Set phenotype-gene edges\n",
        "    data['gene', 'PPI', 'gene'].edge_index = edge_index_gg # Set gene-gene edges\n",
        "\n",
        "    data = transform(data) # Set reverse edges.\n",
        "    data.validate(raise_on_error=True)  # Check the graph structure\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mshEv42W4mhX"
      },
      "source": [
        "Print the topological features of the heterogeneous graph (without including features)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtyApQgS4Y2Q"
      },
      "outputs": [],
      "source": [
        "print(rebuild_graph(df, drug_mapping, gene_mapping, disease_mapping, _, _, _,  drug_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71qX6uZ5_Va0"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, data, x_dict, edge_index_dict):\n",
        "    # sets the model to training mode.\n",
        "    model.train().to(device)\n",
        "    # Before a new forward and backward pass, we need to ensures that gradients from the previous iterations don't accumulate.\n",
        "    optimizer.zero_grad()\n",
        "    # Performs a forward pass through the model.\n",
        "    out = model(x_dict, edge_index_dict)\n",
        "\n",
        "    # Flatten the output and labels for BCELoss\n",
        "    out = out[data['drug'].train_mask].squeeze()\n",
        "    labels = data['drug'].y[data['drug'].train_mask].type_as(out)\n",
        "\n",
        "    criterion = torch.nn.BCELoss()\n",
        "    loss = criterion(out, labels)\n",
        "\n",
        "    loss.backward() # computes the gradients of the loss with respect to the model parameters. It's the step where backpropagation happens.\n",
        "    clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "    optimizer.step() # updates the model's parameters based on the computed gradients.\n",
        "\n",
        "    # Balanced Accuracy Calculation\n",
        "    y_true_train = labels.cpu().detach().numpy()\n",
        "    y_pred_train = (out.cpu().detach().numpy() > 0.5).astype(int) # Threshold to get 0 or 1 as class labels\n",
        "\n",
        "    balanced_acc_train = balanced_accuracy_score(y_true_train, y_pred_train)\n",
        "\n",
        "    correct = y_pred_train == y_true_train\n",
        "    acc_train = correct.sum() / len(y_true_train)\n",
        "\n",
        "    return float(loss), acc_train, balanced_acc_train\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data, x_dict, edge_index_dict, mask_type='valid'):\n",
        "    model.eval().to(device)\n",
        "    out = model(x_dict, edge_index_dict)\n",
        "\n",
        "    # Choose the correct mask based on the function call\n",
        "    if mask_type == 'valid':\n",
        "        mask = data['drug'].val_mask\n",
        "    elif mask_type == 'test':\n",
        "        mask = data['drug'].test_mask\n",
        "    else:\n",
        "        raise ValueError(\"Invalid mask type specified\")\n",
        "\n",
        "    labels = data['drug'].y[mask].type_as(out)\n",
        "    y_true = labels.cpu().detach().numpy()\n",
        "    y_true = y_true.astype('int32')\n",
        "\n",
        "    masked_out = out[mask]\n",
        "    y_pred_prob = masked_out.cpu().detach().numpy().flatten()\n",
        "\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "    y_pred = y_pred.flatten()\n",
        "\n",
        "    # Compute metrics\n",
        "    f_score = f1_score(y_true, y_pred, average='binary')\n",
        "    recall = recall_score(y_true, y_pred, average='binary')\n",
        "    precision = precision_score(y_true, y_pred, average='binary')\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_prob)\n",
        "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "    correct = y_pred == y_true\n",
        "    acc = correct.sum() / len(y_true)\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
        "\n",
        "    return acc, balanced_acc, recall, precision, f_score, roc_auc, fpr, tpr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bnEtcak_H6q"
      },
      "source": [
        "## DruGNNosis-MoA Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdLyz9m_3B7"
      },
      "source": [
        "DruGNNosis-MoA object model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdPKiivD5K5P"
      },
      "outputs": [],
      "source": [
        "class HeteroSciGNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HeteroSciGNN, self).__init__()\n",
        "        # in_channels parameter specifies the dimensionality of the input features of the source nodes.\n",
        "        input_dim = 128\n",
        "        layer1_output_dim = 512\n",
        "        model_output_dim = 1\n",
        "\n",
        "        # First layer with SAGEConv.\n",
        "        self.layer1 = HeteroConv({\n",
        "            ('drug', 'drug_target', 'gene'): SAGEConv(in_channels=input_dim, out_channels=layer1_output_dim),\n",
        "            ('disease', 'disease_gene', 'gene'): SAGEConv(in_channels=input_dim, out_channels=layer1_output_dim),\n",
        "            ('gene', 'PPI', 'gene'): SAGEConv(in_channels=input_dim, out_channels=layer1_output_dim),\n",
        "            ('gene', 'rev_drug_target', 'drug'): SAGEConv(in_channels=input_dim, out_channels=layer1_output_dim),\n",
        "            ('gene', 'rev_disease_gene', 'disease'): SAGEConv(in_channels=input_dim, out_channels=layer1_output_dim)\n",
        "        })\n",
        "\n",
        "        # Second layer with SAGEConv.\n",
        "        self.layer2 = HeteroConv({\n",
        "            ('drug', 'drug_target', 'gene'): SAGEConv(in_channels=layer1_output_dim, out_channels=model_output_dim),\n",
        "            ('disease', 'disease_gene', 'gene'): SAGEConv(in_channels=layer1_output_dim, out_channels=model_output_dim),\n",
        "            ('gene', 'PPI', 'gene'): SAGEConv(in_channels=layer1_output_dim, out_channels=model_output_dim),\n",
        "            ('gene', 'rev_drug_target', 'drug'): SAGEConv(in_channels=layer1_output_dim, out_channels=model_output_dim),\n",
        "            ('gene', 'rev_disease_gene', 'disease'): SAGEConv(in_channels=layer1_output_dim, out_channels=model_output_dim)\n",
        "        })\n",
        "\n",
        "        self.sigmoid = torch.nn.Sigmoid() # Sigmoid activation for binary classification [0, 1].\n",
        "        self.relu = torch.nn.ReLU() # Non linear activation function.\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict, return_intermediate=False):\n",
        "        \"\"\"\n",
        "        x_dict -  A dictionary where keys are node types (e.g., 'drug', 'gene', 'disease') and values are node feature tensors.\n",
        "        edge_index_dict - A dictionary where keys are edge relations (e.g., ('drug', 'drug_target', 'gene'))\n",
        "                            and values are edge index tensors indicating source and destination nodes for each edge.\n",
        "        \"\"\"\n",
        "\n",
        "        # First layer with dropout\n",
        "        x_dict = self.layer1(x_dict, edge_index_dict)\n",
        "        for key in x_dict:\n",
        "            x_dict[key] = self.dropout(self.relu(x_dict[key]))\n",
        "\n",
        "\n",
        "        if return_intermediate: # Used for T-SNE\n",
        "            # x_dict = self.layerTSNE(x_dict, edge_index_dict)\n",
        "            return x_dict['drug']\n",
        "\n",
        "\n",
        "        # Second layer with dropout\n",
        "        x_dict = self.layer2(x_dict, edge_index_dict)\n",
        "\n",
        "\n",
        "        # Directly use the output from layer2\n",
        "        out = self.sigmoid(x_dict['drug'])\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RafFqbPoA09X"
      },
      "source": [
        "Text embedding using the finetuned SciBert per fold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gv-3xYoBE8f"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# --------------- Extract Embeddings with Current Fine-Tuned Model ---------------\n",
        "\"\"\"\n",
        "# Load fold_indexes from the file\n",
        "with open(cv_index_file_path, 'rb') as f:\n",
        "    fold_indexes = pickle.load(f)\n",
        "\n",
        "for fold, _ in enumerate(fold_indexes):\n",
        "    print(f\"Embeddings for fold {fold}: \")\n",
        "\n",
        "    # Generate the directory and compressed model for current fold\n",
        "    dir_fine_tuned_scibert_current_fold = model_dir_format.format(fold)\n",
        "    compressed_model_name = compressed_model_format.format(fold)\n",
        "\n",
        "    # Decompress the model\n",
        "    with tarfile.open(compressed_model_name, 'r:gz') as tar:\n",
        "        tar.extractall()\n",
        "\n",
        "    # Load the fine-tuned model weights into a base AutoModel\n",
        "    model_LLM = AutoModel.from_pretrained(dir_fine_tuned_scibert_current_fold)\n",
        "\n",
        "    # Initialize lists to store embeddings\n",
        "    drug_embeddings = []\n",
        "\n",
        "    for drug_description in tqdm(drug_info_df['Drug_Description'], desc=\"Generate Drug Embedding: \"):\n",
        "        embedding = extract_embeddings(model_LLM, tokenizer, drug_description)\n",
        "        drug_embeddings.append(embedding)\n",
        "\n",
        "    description_embeddings_tensor = torch.cat(drug_embeddings, dim=0)\n",
        "\n",
        "    gene_embeddings = []\n",
        "\n",
        "    # Generate gene embeddings\n",
        "    for gene in tqdm(all_genes, desc=\"Generate Gene Embedding: \"):\n",
        "        embedding = extract_embeddings(model_LLM, tokenizer, gene)\n",
        "        gene_embeddings.append(embedding)\n",
        "\n",
        "    # Convert list of tensors to a single tensor\n",
        "    gene_embeddings_tensor = torch.cat(gene_embeddings, dim=0)\n",
        "\n",
        "    disease_embeddings = []\n",
        "\n",
        "    # Generate disease embeddings\n",
        "    for disease in tqdm(unique_diseases, desc=\"Generate Disease Embedding: \"):\n",
        "        embedding = extract_embeddings(model_LLM, tokenizer, disease)\n",
        "        disease_embeddings.append(embedding)\n",
        "\n",
        "    # Convert list of tensors to a single tensor\n",
        "    disease_embeddings_tensor = torch.cat(disease_embeddings, dim=0)\n",
        "\n",
        "    # Applying PCA to reduce the dimensionality of embeddings\n",
        "    gene_embeddings_reduced = apply_pca(gene_embeddings_tensor, reduced_embedding_dim)\n",
        "    disease_embeddings_reduced = apply_pca(disease_embeddings_tensor, reduced_embedding_dim)\n",
        "    drug_embeddings_reduced = apply_pca(description_embeddings_tensor, reduced_embedding_dim)\n",
        "\n",
        "    # Updating the graph with reduced embeddings\n",
        "    gene_features_pca = torch.tensor(gene_embeddings_reduced, dtype=torch.float32)\n",
        "    disease_features_pca = torch.tensor(disease_embeddings_reduced, dtype=torch.float32)\n",
        "    drug_features_pca = torch.tensor(drug_embeddings_reduced, dtype=torch.float32)\n",
        "\n",
        "    # Combine embeddings into a dictionary\n",
        "    embeddings_dict = {\n",
        "        'gene_embeddings': gene_features_pca,\n",
        "        'disease_embeddings': disease_features_pca,\n",
        "        'drug_embeddings': drug_features_pca\n",
        "    }\n",
        "\n",
        "    text_embeddings_file_path = text_embeddings_file_path_format.format(fold)\n",
        "\n",
        "    # Save the dictionary to a file\n",
        "    with open(text_embeddings_file_path, 'wb') as f:\n",
        "        pickle.dump(embeddings_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-JhN8zXBl-i"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e026Ex9_Bi4H"
      },
      "outputs": [],
      "source": [
        "# Load fold_indexes from the file\n",
        "with open(cv_index_file_path, 'rb') as f:\n",
        "    fold_indexes = pickle.load(f)\n",
        "\n",
        "# Initialize a DataFrame to hold all results\n",
        "hetroSciGNN_cv_df = pd.DataFrame()\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(fold_indexes):\n",
        "\n",
        "    text_embeddings_file_path = text_embeddings_file_path_format.format(fold)\n",
        "\n",
        "    # Load the embeddings dictionary\n",
        "    with open(text_embeddings_file_path, 'rb') as f:\n",
        "        embeddings_dict = pickle.load(f)\n",
        "\n",
        "    # Access the embeddings\n",
        "    gene_features_pca = embeddings_dict['gene_embeddings']\n",
        "    disease_features_pca = embeddings_dict['disease_embeddings']\n",
        "    drug_features_pca = embeddings_dict['drug_embeddings']\n",
        "\n",
        "    # Rebuild the graph for the current fold\n",
        "    hetroSciGNN_data = rebuild_graph(df, drug_mapping, gene_mapping, disease_mapping, drug_features_pca, gene_features_pca, disease_features_pca,  drug_y)\n",
        "    hetroSciGNN_model = HeteroSciGNN()\n",
        "\n",
        "    # The Adam optimizer is initialized with a learning rate.\n",
        "    hetroSciGNN_optimizer = Adam(hetroSciGNN_model.parameters(), lr=0.0006)\n",
        "\n",
        "    hetroSciGNN_data = hetroSciGNN_data.to(device)\n",
        "    hetroSciGNN_model = hetroSciGNN_model.to(device) # Move the hetroSciGNN_model to the available device (either GPU or CPU).\n",
        "\n",
        "    # Create masks for the current fold\n",
        "    train_mask = torch.zeros(num_nodes_drugs, dtype=torch.bool)\n",
        "    val_mask = torch.zeros(num_nodes_drugs, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(num_nodes_drugs, dtype=torch.bool)\n",
        "\n",
        "    test_mask[test_indices] = True\n",
        "    train_mask[train_idx] = True\n",
        "    val_mask[val_idx] = True\n",
        "\n",
        "    # Update the data object with train, validation, and test masks for drug nodes\n",
        "    hetroSciGNN_data['drug'].train_mask = train_mask\n",
        "    hetroSciGNN_data['drug'].val_mask = val_mask\n",
        "    hetroSciGNN_data['drug'].test_mask = test_mask\n",
        "\n",
        "    # Train and validate the hetroSciGNN_model using the current fold\n",
        "    for epoch in range(1, 36):\n",
        "        loss, acc_train, balanced_acc_train = train(hetroSciGNN_model, hetroSciGNN_optimizer, hetroSciGNN_data, hetroSciGNN_data.x_dict, hetroSciGNN_data.edge_index_dict)\n",
        "        acc_v, balanced_acc_v, recall_v, precision_v, f1_v, roc_auc_v, fpr_v, tpr_v = evaluate(hetroSciGNN_model, hetroSciGNN_data, hetroSciGNN_data.x_dict, hetroSciGNN_data.edge_index_dict, mask_type='valid')\n",
        "        print(f'Fold: {fold}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Acc_train: {acc_train:.4f}, Acc_valid: {acc_v:.4f}, F1-Score_valid: {f1_v:.4f}, Roc-Auc_valid: {roc_auc_v:.4f}')\n",
        "        if (f1_v >= 0.95):\n",
        "            break\n",
        "\n",
        "    # Save metrics for the validation set of the current fold.\n",
        "    hetroSciGNN_fold_metrics_valid = {\n",
        "        'fold': fold,\n",
        "        'acc_v': acc_v,\n",
        "        'balanced_acc_v': balanced_acc_v,\n",
        "        'recall_v': recall_v,\n",
        "        'precision_v': precision_v,\n",
        "        'f1_v': f1_v,\n",
        "        'roc_auc_v': roc_auc_v,\n",
        "        'fpr': fpr_v,\n",
        "        'tpr': tpr_v\n",
        "    }\n",
        "\n",
        "    hetroSciGNN_fold_results = pd.DataFrame([hetroSciGNN_fold_metrics_valid])\n",
        "    hetroSciGNN_cv_df = pd.concat([hetroSciGNN_cv_df, hetroSciGNN_fold_results], ignore_index=True)\n",
        "\n",
        "    # After using the model and data\n",
        "    del hetroSciGNN_model  # Delete the model\n",
        "    del hetroSciGNN_data   # Delete the data\n",
        "\n",
        "    # Clear the CUDA cache if using GPU\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Explicitly invoke garbage collection.\n",
        "    gc.collect()\n",
        "\n",
        "# Save the results DataFrame to a file\n",
        "hetroSciGNN_cv_df.to_pickle(hetroSciGNN_cv_results_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDeRzTNY87mn"
      },
      "source": [
        "## Baseline GNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdCNVUHO_xd3"
      },
      "source": [
        "baseGNN object model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX7KXCep_w0G"
      },
      "outputs": [],
      "source": [
        "class baseGNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(baseGNN, self).__init__()\n",
        "\n",
        "        input_dim = 16\n",
        "        layer1_output_dim = 256\n",
        "        model_output_dim = 1\n",
        "\n",
        "        # First layer with RGCNConv.\n",
        "        self.layer1 = HeteroConv({\n",
        "            ('drug', 'drug_target', 'gene'): SAGEConv(in_channels=input_dim, out_channels=layer1_output_dim),\n",
        "            ('disease', 'disease_gene', 'gene'): SAGEConv(in_channels=input_dim, out_channels=layer1_output_dim),\n",
        "            ('gene', 'PPI', 'gene'): SAGEConv(in_channels=input_dim, out_channels=layer1_output_dim),\n",
        "            ('gene', 'rev_drug_target', 'drug'): SAGEConv(in_channels=input_dim, out_channels=layer1_output_dim),\n",
        "            ('gene', 'rev_disease_gene', 'disease'): SAGEConv(in_channels=input_dim, out_channels=layer1_output_dim)\n",
        "        })\n",
        "        self.layer2 = HeteroConv({\n",
        "            ('drug', 'drug_target', 'gene'): SAGEConv(in_channels=layer1_output_dim, out_channels=model_output_dim),\n",
        "            ('disease', 'disease_gene', 'gene'): SAGEConv(in_channels=layer1_output_dim, out_channels=model_output_dim),\n",
        "            ('gene', 'PPI', 'gene'): SAGEConv(in_channels=layer1_output_dim, out_channels=model_output_dim),\n",
        "            ('gene', 'rev_drug_target', 'drug'): SAGEConv(in_channels=layer1_output_dim, out_channels=model_output_dim),\n",
        "            ('gene', 'rev_disease_gene', 'disease'): SAGEConv(in_channels=layer1_output_dim, out_channels=model_output_dim)\n",
        "        })\n",
        "\n",
        "        self.sigmoid = torch.nn.Sigmoid() # Sigmoid activation for binary classification [0, 1].\n",
        "        self.relu = torch.nn.ReLU() # Non linear activation function.\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict, return_intermediate=False):\n",
        "        \"\"\"\n",
        "        x_dict -  A dictionary where keys are node types (e.g., 'drug', 'gene', 'disease') and values are node feature tensors.\n",
        "        edge_index_dict - A dictionary where keys are edge relations (e.g., ('drug', 'drug_target', 'gene'))\n",
        "                            and values are edge index tensors indicating source and destination nodes for each edge.\n",
        "        \"\"\"\n",
        "\n",
        "        # First layer\n",
        "        x_dict = self.layer1(x_dict, edge_index_dict)\n",
        "        for key in x_dict:\n",
        "            x_dict[key] = self.relu(x_dict[key])\n",
        "\n",
        "        # Second layer\n",
        "        x_dict = self.layer2(x_dict, edge_index_dict)\n",
        "\n",
        "        # Directly use the output from layer4\n",
        "        out = self.sigmoid(x_dict['drug'])\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkPuKLBU_mGW"
      },
      "source": [
        " Embedding Eigen Vector for the baseline gnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlzCbQd49BSG"
      },
      "outputs": [],
      "source": [
        "# Calculating the total number of nodes in the graph\n",
        "total_nodes = len(drug_mapping) + len(disease_mapping) + len(gene_mapping)\n",
        "combined_adj_matrix = torch.zeros((total_nodes, total_nodes))\n",
        "\n",
        "# Adding Drug-Gene edges\n",
        "for src, tgt in tqdm(zip(source_nodes_dg, target_nodes_dg), total=len(source_nodes_dg), desc=\"Adding Drug-Gene Edges\"):\n",
        "    # Since drug nodes come first, their indices are unchanged\n",
        "    # Gene indices need to be offset by the number of drug and phenotype nodes\n",
        "    combined_adj_matrix[src, len(drug_mapping) + len(disease_mapping) + tgt] = 1\n",
        "    combined_adj_matrix[len(drug_mapping) + len(disease_mapping) + tgt, src] = 1\n",
        "\n",
        "# Adding Phenotype-Gene edges\n",
        "for src, tgt in tqdm(zip(source_nodes_pg, target_nodes_pg), total=len(source_nodes_pg), desc=\"Adding Phenotype-Gene Edges\"):\n",
        "    # Offset source index for phenotypes by the number of drugs\n",
        "    # Offset target index for genes as before\n",
        "    combined_adj_matrix[len(drug_mapping) + src, len(drug_mapping) + len(disease_mapping) + tgt] = 1\n",
        "    combined_adj_matrix[len(drug_mapping) + len(disease_mapping) + tgt, len(drug_mapping) + src] = 1\n",
        "\n",
        "# Adding Gene-Gene edges (PPI)\n",
        "for src, tgt in tqdm(zip(source_nodes_gg, target_nodes_gg), total=len(source_nodes_gg), desc=\"Adding Gene-Gene Edges\"):\n",
        "    # Both source and target indices need to be offset for genes\n",
        "    combined_adj_matrix[len(drug_mapping) + len(disease_mapping) + src, len(drug_mapping) + len(disease_mapping) + tgt] = 1\n",
        "    combined_adj_matrix[len(drug_mapping) + len(disease_mapping) + tgt, len(drug_mapping) + len(disease_mapping) + src] = 1\n",
        "\n",
        "def compute_eigen_features(adj_matrix, k=16):\n",
        "    # Convert to CSR format for efficient computation\n",
        "    adj_matrix_csr = csr_matrix(adj_matrix.numpy())\n",
        "\n",
        "    # Compute the top k eigenvalues and eigenvectors\n",
        "    _, eigenvectors = eigs(adj_matrix_csr, k=k, which='LM')\n",
        "    eigenvectors = eigenvectors.real  # Take the real values\n",
        "\n",
        "    # Z-score normalization\n",
        "    eigenvector_means = np.mean(eigenvectors, axis=0)\n",
        "    eigenvector_stds = np.std(eigenvectors, axis=0)\n",
        "    normalized_eigenvectors = (eigenvectors - eigenvector_means) / eigenvector_stds\n",
        "\n",
        "    return torch.from_numpy(normalized_eigenvectors)\n",
        "\n",
        "# Compute eigen features for the combined graph\n",
        "combined_eigen_features = compute_eigen_features(combined_adj_matrix)\n",
        "\n",
        "torch.save(combined_eigen_features, eigen_featues_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndjS05RLCGwE"
      },
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58SLCqdPCE-t"
      },
      "outputs": [],
      "source": [
        "# Load fold_indexes from the file\n",
        "with open(cv_index_file_path, 'rb') as f:\n",
        "    fold_indexes = pickle.load(f)\n",
        "\n",
        "# Initialize a DataFrame to hold all results\n",
        "baseGNN_cv_df = pd.DataFrame()\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(fold_indexes):\n",
        "\n",
        "    combined_eigen_features = torch.load(eigen_featues_path)\n",
        "    # Assign eigen features to each node type\n",
        "    drug_baseModel_features = combined_eigen_features[:len(drug_mapping), :]\n",
        "    disease_baseModel_features = combined_eigen_features[len(drug_mapping):(len(drug_mapping) + len(disease_mapping)), :]\n",
        "    gene_baseModel_features = combined_eigen_features[(len(drug_mapping) + len(disease_mapping)):, :]\n",
        "\n",
        "    # Rebuild the graph for the current fold\n",
        "    baseGNN_data = rebuild_graph(df, drug_mapping, gene_mapping, disease_mapping, drug_baseModel_features, gene_baseModel_features, disease_baseModel_features, drug_y)\n",
        "    baseGNN_model = baseGNN()\n",
        "\n",
        "    # The Adam optimizer is initialized with a learning rate.\n",
        "    baseGNN_optimizer = Adam(baseGNN_model.parameters(), lr=0.0005)\n",
        "\n",
        "    print(torch.cuda.is_available())\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Determine whether a GPU is available for computation.\n",
        "    baseGNN_data = baseGNN_data.to(device)\n",
        "    baseGNN_model = baseGNN_model.to(device) # Move the model to the available device (either GPU or CPU).\n",
        "\n",
        "    # Create masks for the current fold\n",
        "    train_mask = torch.zeros(num_nodes_drugs, dtype=torch.bool)\n",
        "    val_mask = torch.zeros(num_nodes_drugs, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(num_nodes_drugs, dtype=torch.bool)\n",
        "\n",
        "    test_mask[test_indices] = True\n",
        "    train_mask[train_idx] = True\n",
        "    val_mask[val_idx] = True\n",
        "\n",
        "    # Update the data object with train, validation, and test masks for drug nodes\n",
        "    baseGNN_data['drug'].train_mask = train_mask\n",
        "    baseGNN_data['drug'].val_mask = val_mask\n",
        "    baseGNN_data['drug'].test_mask = test_mask\n",
        "\n",
        "    # Train and validate the model using the current fold\n",
        "    for epoch in range(1, 101):\n",
        "        loss, acc_train, balanced_acc_train = train(baseGNN_model, baseGNN_optimizer, baseGNN_data, baseGNN_data.x_dict, baseGNN_data.edge_index_dict)\n",
        "        acc_v, balanced_acc_v, recall_v, precision_v, f1_v, roc_auc_v, fpr_v, tpr_v = evaluate(baseGNN_model, baseGNN_data, baseGNN_data.x_dict, baseGNN_data.edge_index_dict, mask_type='valid')\n",
        "        print(f'Fold: {fold}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Acc_train: {acc_train:.4f}, Acc_valid: {acc_v:.4f}, F1-Score_valid: {f1_v:.4f}, Roc-Auc_valid: {roc_auc_v:.4f}')\n",
        "\n",
        "    # Save metrics for the validation set of the current fold.\n",
        "    baseGNN_fold_metrics_valid = {\n",
        "        'fold': fold,\n",
        "        'acc_v': acc_v,\n",
        "        'balanced_acc_v': balanced_acc_v,\n",
        "        'recall_v': recall_v,\n",
        "        'precision_v': precision_v,\n",
        "        'f1_v': f1_v,\n",
        "        'roc_auc_v': roc_auc_v,\n",
        "        'fpr': fpr_v,\n",
        "        'tpr': tpr_v\n",
        "    }\n",
        "\n",
        "    # Inside your fold loop\n",
        "    baseGNN_fold_results = pd.DataFrame([baseGNN_fold_metrics_valid])\n",
        "    baseGNN_cv_df = pd.concat([baseGNN_cv_df, baseGNN_fold_results], ignore_index=True)\n",
        "\n",
        "\n",
        "    del baseGNN_model  # Delete the model\n",
        "    del baseGNN_data   # Delete the data\n",
        "\n",
        "    # Clear the CUDA cache if using GPU\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Explicitly invoke garbage collection\n",
        "    gc.collect()\n",
        "\n",
        "# Save the DataFrame to a file\n",
        "baseGNN_cv_df.to_pickle(baseGNN_cv_results_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T13cvIAuDMQH"
      },
      "source": [
        "# 5. Compute the average on the validtion set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGJHiwCvLl8a"
      },
      "source": [
        "SciBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fukq94VJJ7IP"
      },
      "outputs": [],
      "source": [
        "# Load the saved metrics\n",
        "with open(SciBERT_valid_fold_metrics_path, 'rb') as file:\n",
        "    SciBERT_valid_metrics_loaded = pickle.load(file)\n",
        "\n",
        "# Extract the list of dictionaries containing the metrics\n",
        "metrics_list = SciBERT_valid_metrics_loaded['metrics']\n",
        "\n",
        "# Convert loaded metrics to a DataFrame\n",
        "metrics_df = pd.DataFrame(metrics_list)\n",
        "\n",
        "# Print metrics for each fold for validation set in the desired format\n",
        "print(\"Validation Metrics for each fold: \\n\")\n",
        "for i, row in metrics_df.iterrows():\n",
        "    print(f\"Fold {i+1}: \", end='')\n",
        "    print(', '.join([f\"{metric}: {row[metric]:.4f}\" for metric in metrics_df.columns]))\n",
        "\n",
        "# Calculate average and standard deviation of metrics across folds for validation set\n",
        "average_metrics = metrics_df.mean()\n",
        "std_metrics = metrics_df.std()\n",
        "\n",
        "# Print average and standard deviation of metrics in the desired format\n",
        "print(\"\\nAverage Validation Metrics Across All Folds:\")\n",
        "print(', '.join([f\"\\n Average {metric}: {average_metrics[metric]:.4f}\" for metric in average_metrics.index]))\n",
        "\n",
        "print(\"\\nStandard Deviation of Validation Metrics Across All Folds:\")\n",
        "print(', '.join([f\"\\n Std {metric}: {std_metrics[metric]:.4f}\" for metric in std_metrics.index]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyA5_qHJLq7t"
      },
      "source": [
        "DruGNNosis-MoA Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q_N2Wmu8__Z"
      },
      "outputs": [],
      "source": [
        "# Load validation fold metrics\n",
        "hetroSciGNN_cv_df = pd.read_pickle(hetroSciGNN_cv_results_path)\n",
        "\n",
        "# After the loop, print metrics for each fold\n",
        "print(\"Metrics for each fold:\")\n",
        "for index, row in hetroSciGNN_cv_df.iterrows():\n",
        "    # Exclude the first 'fold' column and the last two columns from the row\n",
        "    metrics_to_print = row[1:-2]\n",
        "    print(f'Fold {index + 1}:', ', '.join([f'{key}: {value:.4f}' for key, value in metrics_to_print.items() if key != 'fold']))\n",
        "\n",
        "# Calculate the average and standard deviation of metrics (only numeric columns)\n",
        "average_metrics = hetroSciGNN_cv_df.mean(numeric_only=True)\n",
        "std_metrics = hetroSciGNN_cv_df.std(numeric_only=True)\n",
        "\n",
        "# Print the average results\n",
        "print(\"\\nAverage validation metrics across all folds:\")\n",
        "print(average_metrics[1:])\n",
        "\n",
        "# Print the standard deviation of metrics\n",
        "print(\"\\nStandard deviation of validation metrics across all folds:\")\n",
        "print(std_metrics[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed-XyDPHLzTI"
      },
      "source": [
        "BaseGNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDGwDqDmw3Fa"
      },
      "outputs": [],
      "source": [
        "# Load validation fold metrics\n",
        "baseGNN_cv_df = pd.read_pickle(baseGNN_cv_results_path)\n",
        "\n",
        "# After the loop, print metrics for each fold\n",
        "print(\"Metrics for each fold:\")\n",
        "for index, row in baseGNN_cv_df.iterrows():\n",
        "    # Exclude the first 'fold' column and the last two columns from the row\n",
        "    metrics_to_print = row[1:-2]\n",
        "    print(f'Fold {index + 1}:', ', '.join([f'{key}: {value:.4f}' for key, value in metrics_to_print.items() if key != 'fold']))\n",
        "\n",
        "# Calculate the average and standard deviation of metrics (only numeric columns)\n",
        "average_metrics = baseGNN_cv_df.mean(numeric_only=True)\n",
        "std_metrics = baseGNN_cv_df.std(numeric_only=True)\n",
        "\n",
        "# Print the average results\n",
        "print(\"\\nAverage validation metrics across all folds:\")\n",
        "print(average_metrics[1:])\n",
        "\n",
        "# Print the standard deviation of metrics\n",
        "print(\"\\nStandard deviation of validation metrics across all folds:\")\n",
        "print(std_metrics[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Best model Training and Evaluation"
      ],
      "metadata": {
        "id": "Pyeo5ng3vtls"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umLH5f4pQ0Pt"
      },
      "source": [
        "## Train DruGNNosis-MoA model without validtion set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmFowf6PZ16D"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Determine whether a GPU is available for computation.\n",
        "\n",
        "# Tokenization - Using an access token is optional for public models - include it with token=access_token if you have one, or omit it otherwise.\n",
        "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', token=access_token)\n",
        "\n",
        "# Initialize pre-trained model for scientific text.\n",
        "model_LLM_embedding = AutoModelForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels=2, token=access_token)\n",
        "\n",
        "# Extract the \"Drug_Description\".\n",
        "drug_info_df['text'] = drug_info_df['Drug_Description'].fillna('')\n",
        "\n",
        "train_val_df = drug_info_df.iloc[train_val_indices].copy() # Create the train_valid df with the same 'test_indices' we used for the GNN.\n",
        "\n",
        "train_val_df.rename(columns={'MoA': 'label'}, inplace=True)\n",
        "\n",
        "# Create train data set without leaving data for validtion.\n",
        "train90_dataset = Dataset.from_pandas(train_val_df).map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr5_rYVXASjo"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "  output_dir=f'./llm_model_output',\n",
        "  learning_rate=2e-5,\n",
        "  per_device_train_batch_size=16,\n",
        "  per_device_eval_batch_size=16,\n",
        "  num_train_epochs=3,\n",
        "  weight_decay=0.002,\n",
        "  evaluation_strategy=\"no\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "  model=model_LLM_embedding,\n",
        "  args=training_args,\n",
        "  train_dataset=train90_dataset,\n",
        "  tokenizer=tokenizer,\n",
        "  compute_metrics=None\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "model_LLM_embedding.save_pretrained(fine_tuned_90DataSet_path)\n",
        "\n",
        "# Create a tar.gz file\n",
        "with tarfile.open(compressed_bestModel_format, \"w:gz\") as tar:\n",
        "    tar.add(fine_tuned_90DataSet_path, arcname=os.path.basename(fine_tuned_90DataSet_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTYS45OsZf4H"
      },
      "source": [
        " Extract Embeddings with The Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLdB4hW7AU1a"
      },
      "outputs": [],
      "source": [
        "print('\\n Calculate Text Embedding For The SciBretGNN Model: \\n')\n",
        "\n",
        "# Load the fine-tuned model weights into a base AutoModel\n",
        "model_LLM_embedding = AutoModel.from_pretrained(fine_tuned_90DataSet_path)\n",
        "\n",
        "# Initialize lists to store embeddings\n",
        "drug_embeddings = []\n",
        "\n",
        "for drug_description in tqdm(drug_info_df['Drug_Description'], desc=\"Generate Drug Embedding: \"):\n",
        "    embedding = extract_embeddings(model_LLM_embedding, tokenizer, drug_description)\n",
        "    drug_embeddings.append(embedding)\n",
        "\n",
        "description_embeddings_tensor = torch.cat(drug_embeddings, dim=0)\n",
        "\n",
        "gene_embeddings = []\n",
        "\n",
        "# Generate gene embeddings\n",
        "for gene in tqdm(all_genes, desc=\"Generate Gene Embedding: \"):\n",
        "    embedding = extract_embeddings(model_LLM_embedding, tokenizer, gene)\n",
        "    gene_embeddings.append(embedding)\n",
        "\n",
        "# Convert list of tensors to a single tensor\n",
        "gene_embeddings_tensor = torch.cat(gene_embeddings, dim=0)\n",
        "\n",
        "disease_embeddings = []\n",
        "\n",
        "# Generate disease embeddings\n",
        "for disease in tqdm(unique_diseases, desc=\"Generate Disease Embedding: \"):\n",
        "    embedding = extract_embeddings(model_LLM_embedding, tokenizer, disease)\n",
        "    disease_embeddings.append(embedding)\n",
        "\n",
        "# Convert list of tensors to a single tensor\n",
        "disease_embeddings_tensor = torch.cat(disease_embeddings, dim=0)\n",
        "\n",
        "# Applying PCA to reduce the dimensionality of embeddings\n",
        "gene_embeddings_reduced = apply_pca(gene_embeddings_tensor, reduced_embedding_dim)\n",
        "disease_embeddings_reduced = apply_pca(disease_embeddings_tensor, reduced_embedding_dim)\n",
        "drug_embeddings_reduced = apply_pca(description_embeddings_tensor, reduced_embedding_dim)\n",
        "\n",
        "# Updating the graph with reduced embeddings\n",
        "gene_features_pca = torch.tensor(gene_embeddings_reduced, dtype=torch.float32)\n",
        "disease_features_pca = torch.tensor(disease_embeddings_reduced, dtype=torch.float32)\n",
        "drug_features_pca = torch.tensor(drug_embeddings_reduced, dtype=torch.float32)\n",
        "\n",
        "# Save the embeddings\n",
        "torch.save(gene_features_pca, best_model_gene_features_path)\n",
        "torch.save(disease_features_pca, best_model_disease_features_path)\n",
        "torch.save(drug_features_pca, best_model_drug_features_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXfuYWWSZUJN"
      },
      "source": [
        "DruGNNosis-MoA Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyBxH2XUlXdO"
      },
      "outputs": [],
      "source": [
        "bestModel_gene_features = torch.load(best_model_gene_features_path)\n",
        "bestModel_disease_features = torch.load(best_model_disease_features_path)\n",
        "bestModel_drug_features = torch.load(best_model_drug_features_path)\n",
        "\n",
        "# Rebuild the graph\n",
        "hetroSciGNN_data = rebuild_graph(df, drug_mapping, gene_mapping, disease_mapping, bestModel_drug_features, bestModel_gene_features, bestModel_disease_features,  drug_y)\n",
        "hetroSciGNN_model = HeteroSciGNN()\n",
        "\n",
        "# The Adam optimizer is initialized with a learning rate.\n",
        "hetroSciGNN_optimizer = Adam(hetroSciGNN_model.parameters(), lr=0.0006)\n",
        "\n",
        "hetroSciGNN_data = hetroSciGNN_data.to(device)\n",
        "hetroSciGNN_model = hetroSciGNN_model.to(device) # Move the hetroSciGNN_model to the available device (either GPU or CPU).\n",
        "\n",
        "# Create masks\n",
        "train_mask = torch.zeros(num_nodes_drugs, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_nodes_drugs, dtype=torch.bool)\n",
        "\n",
        "test_mask[test_indices] = True\n",
        "train_mask[train_val_indices] = True\n",
        "\n",
        "# Update the data object with train, and test masks for drug nodes\n",
        "hetroSciGNN_data['drug'].train_mask = train_mask\n",
        "hetroSciGNN_data['drug'].test_mask = test_mask\n",
        "\n",
        "# Train the hetroSciGNN model\n",
        "for epoch in range(1, 36):\n",
        "    loss, acc_train, balanced_acc_train = train(hetroSciGNN_model, hetroSciGNN_optimizer, hetroSciGNN_data, hetroSciGNN_data.x_dict, hetroSciGNN_data.edge_index_dict)\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Acc_train: {acc_train:.4f}')\n",
        "\n",
        "# Save the model\n",
        "torch.save(hetroSciGNN_model.state_dict(), best_model_path)\n",
        "\n",
        "# Save the input data\n",
        "with open(best_inputData_path, 'wb') as f:\n",
        "    pickle.dump(hetroSciGNN_data, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFw6WYgfQ9Mp"
      },
      "source": [
        "## Evaluate the DruGNNosis-MoA on the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you run this part separately, <br>\n",
        "please ensure that you have run the 'HeteroSciGNN' class and the evaluate method beforehand and that your file paths are correct."
      ],
      "metadata": {
        "id": "JNEGa-l8bszS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok7qv5v3dSh9"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "hetroSciGNN_model = HeteroSciGNN()  # Initialize the model again\n",
        "hetroSciGNN_model.load_state_dict(torch.load(best_model_path))\n",
        "hetroSciGNN_model = hetroSciGNN_model.to(device)  # Move to the appropriate device\n",
        "\n",
        "# Load the input data\n",
        "with open(best_inputData_path, 'rb') as f:\n",
        "    hetroSciGNN_data = pickle.load(f)\n",
        "\n",
        "acc_t, balanced_acc_t, recall_t, precision_t, f1_t, auc_roc_t, fpr_t, tpr_t = evaluate(hetroSciGNN_model, hetroSciGNN_data, hetroSciGNN_data.x_dict, hetroSciGNN_data.edge_index_dict, mask_type='test')\n",
        "\n",
        "# Save metrics for the test set current fold.\n",
        "hetroSciGNN_metrics_test = {\n",
        "    'acc_t': acc_t,\n",
        "    'balanced_acc_t': balanced_acc_t,\n",
        "    'recall_t': recall_t,\n",
        "    'precision_t': precision_t,\n",
        "    'f1_t': f1_t,\n",
        "    'roc_auc_t': auc_roc_t,\n",
        "    'fpr': fpr_t,\n",
        "    'tpr': tpr_t\n",
        "}\n",
        "\n",
        "print(f'### TEST RESULTS ###')\n",
        "print(f'Acc_test: {acc_t:.4f}, Recall: {recall_t:.4f}, Precision: {precision_t:.4f}, F1: {f1_t:.4f}, AUC: {auc_roc_t:.4f}')\n",
        "\n",
        "# Save test results in a file\n",
        "with open(hetroSciGNN_test_metrics_path, 'wb') as f:\n",
        "    pickle.dump(hetroSciGNN_metrics_test, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GP4kuzKI-6N"
      },
      "source": [
        "# 7. Comparison of F1 and ROC-AUC Scores Across Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrridiZKZTC-"
      },
      "outputs": [],
      "source": [
        "hetroSciGNN_cv_df = pd.read_pickle(hetroSciGNN_cv_results_path) # Load data for hetroSciGNN model\n",
        "baseGNN_cv_df = pd.read_pickle(baseGNN_cv_results_path) # Load data for baseGNN model\n",
        "\n",
        "# Load data for SciBERT model\n",
        "with open(SciBERT_valid_fold_metrics_path, 'rb') as file:\n",
        "    scibert_results = pickle.load(file)\n",
        "\n",
        "# Extract F1 scores from hetroSciGNN\n",
        "hetroSciGNN_f1_scores = hetroSciGNN_cv_df['f1_v']\n",
        "print(\"DruGNNosis-MoA F1 Scores:\")\n",
        "print(hetroSciGNN_f1_scores)\n",
        "\n",
        "# Extract F1 scores from baseGNN\n",
        "baseGNN_f1_scores = baseGNN_cv_df['f1_v']\n",
        "print(\"\\nbaseGNN F1 Scores:\")\n",
        "print(baseGNN_f1_scores)\n",
        "\n",
        "# Extract F1 scores for SciBERT\n",
        "SciBERT_f1_scores = [fold_metrics['eval_f1'] for fold_metrics in scibert_results['metrics']]\n",
        "print(\"SciBERT F1 Scores:\")\n",
        "print(SciBERT_f1_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5ZwhQ6xS6rc"
      },
      "source": [
        "## 7.I. F1 Score Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdNNKz3fi9Wk"
      },
      "outputs": [],
      "source": [
        "# Shapiro-Wilk Test for normality\n",
        "gnn_f1_normality = stats.shapiro(hetroSciGNN_f1_scores)\n",
        "scibert_f1_normality = stats.shapiro(SciBERT_f1_scores)\n",
        "\n",
        "# Print normality test results\n",
        "print(\"\\nGNN F1 Normality Test:\", gnn_f1_normality)\n",
        "print(\"SciBERT F1 Normality Test:\", scibert_f1_normality)\n",
        "\n",
        "# Interpretation of Shapiro-Wilk Test results\n",
        "print(\"\\nInterpretation: GNN F1 scores are\" + (\" not\" if gnn_f1_normality.pvalue < 0.05 else \"\") + \" normally distributed.\")\n",
        "print(\"Interpretation: SciBERT F1 scores are\" + (\" not\" if scibert_f1_normality.pvalue < 0.05 else \"\") + \" normally distributed.\")\n",
        "\n",
        "# Welch's t-test (does not assume equal variances)\n",
        "f1_t_test_result = stats.ttest_ind(hetroSciGNN_f1_scores, SciBERT_f1_scores, equal_var=False)\n",
        "\n",
        "# Print t-test results\n",
        "print(\"\\nF1 Scores t-test Result:\", f1_t_test_result)\n",
        "\n",
        "# Interpretation of t-test results\n",
        "print(\"Interpretation: There is\" + (\" a\" if f1_t_test_result.pvalue < 0.05 else \" no\") + \" statistically significant difference in F1 scores between GNN and SciBERT models.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca4Yx-EYu2Kp"
      },
      "outputs": [],
      "source": [
        "# Sorting scores for CDF\n",
        "gnn_sorted_scores = np.sort(hetroSciGNN_f1_scores)\n",
        "scibert_sorted_scores = np.sort(SciBERT_f1_scores)\n",
        "baseGNN_sorted_scores = np.sort(baseGNN_f1_scores)\n",
        "\n",
        "# Calculating CDF\n",
        "gnn_yvals = np.arange(len(gnn_sorted_scores))/float(len(gnn_sorted_scores)-1)\n",
        "scibert_yvals = np.arange(len(scibert_sorted_scores))/float(len(scibert_sorted_scores)-1)\n",
        "baseGNN_yvals = np.arange(len(baseGNN_sorted_scores))/float(len(baseGNN_sorted_scores)-1)\n",
        "\n",
        "# Plotting CDF\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(gnn_sorted_scores, gnn_yvals, label='DruGNNosis-MoA', marker='o')\n",
        "plt.plot(scibert_sorted_scores, scibert_yvals, label='SciBERT', marker='o')\n",
        "plt.plot(baseGNN_sorted_scores, baseGNN_yvals, label='Base GNN', marker='o')\n",
        "\n",
        "plt.xlabel('F1-score')\n",
        "plt.ylabel('Cumulative Probability')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('Cumulative_Distribution_F1.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmrLjui1zWHb"
      },
      "outputs": [],
      "source": [
        "# The folds\n",
        "folds = np.arange(1, len(hetroSciGNN_f1_scores) + 1)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(folds, baseGNN_sorted_scores, marker='.', lw=3, label='Baseline GNN', linestyle='-',  color='green')\n",
        "plt.plot(folds, SciBERT_f1_scores, marker='x', lw=3, label='Fine-tuned SciBERT', linestyle='-', color='orange')\n",
        "plt.plot(folds, hetroSciGNN_f1_scores, marker='o', lw=3, label='DruGNNosis-MoA', linestyle='-', color='blue')\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.xlabel('Fold Number', fontsize=16)\n",
        "plt.ylabel('F1-score', fontsize=16)\n",
        "plt.xticks(folds)  # Set x-ticks to be fold numbers\n",
        "plt.legend(fontsize=14)\n",
        "\n",
        "plt.grid(True)\n",
        "plt.savefig('Comparison_F1_Scores_3_models.pdf', format='pdf', dpi=1200, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npDQMDPF6BUq"
      },
      "outputs": [],
      "source": [
        "# The folds\n",
        "folds = np.arange(1, len(hetroSciGNN_f1_scores) + 1)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(folds, SciBERT_f1_scores, marker='x', lw=3, label='Fine-tuned SciBERT', linestyle='-', color='orange')\n",
        "plt.plot(folds, hetroSciGNN_f1_scores, marker='o', lw=3, label='DruGNNosis-MoA', linestyle='-', color='blue')\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.xlabel('Fold Number', fontsize=16)\n",
        "plt.ylabel('F1 Score', fontsize=16)\n",
        "plt.xticks(folds)  # Set x-ticks to be fold numbers\n",
        "plt.legend(loc='lower left', fontsize=14)\n",
        "\n",
        "plt.grid(True)\n",
        "plt.savefig('Comparison_F1_Scores_2_models.pdf', format='pdf', dpi=1200, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX_vqwEnmwNm"
      },
      "outputs": [],
      "source": [
        "hetroSciGNN_cv_df = pd.read_pickle(hetroSciGNN_cv_results_path) # Load data for hetroSciGNN model\n",
        "baseGNN_cv_df = pd.read_pickle(baseGNN_cv_results_path) # Load data for baseGNN model\n",
        "\n",
        "# Load data for SciBERT model\n",
        "with open(SciBERT_valid_fold_metrics_path, 'rb') as file:\n",
        "    scibert_results = pickle.load(file)\n",
        "\n",
        "# Extract ROC-AUC scores from the GNN model's test metrics\n",
        "gnn_roc_auc_scores = hetroSciGNN_cv_df['roc_auc_v']\n",
        "\n",
        "# Extract ROC-AUC scores from the SciBERT model's test metrics\n",
        "scibert_roc_auc_scores = np.array([metrics['eval_roc_auc'] for metrics in scibert_results['metrics']])\n",
        "print(scibert_roc_auc_scores)\n",
        "\n",
        "# Perform Shapiro-Wilk Test for normality\n",
        "gnn_roc_auc_normality = stats.shapiro(gnn_roc_auc_scores)\n",
        "scibert_roc_auc_normality = stats.shapiro(scibert_roc_auc_scores)\n",
        "print(\"\\nGNN ROC-AUC Normality Test:\", gnn_roc_auc_normality)\n",
        "print(\"SciBERT ROC-AUC Normality Test:\", scibert_roc_auc_normality)\n",
        "\n",
        "# Interpretation of Shapiro-Wilk Test results\n",
        "print(\"\\nInterpretation: GNN ROC-AUC scores are\" + (\" not\" if gnn_roc_auc_normality.pvalue < 0.05 else \"\") + \" normally distributed.\")\n",
        "print(\"Interpretation: SciBERT ROC-AUC scores are\" + (\" not\" if scibert_roc_auc_normality.pvalue < 0.05 else \"\") + \" normally distributed.\")\n",
        "\n",
        "# Perform Welch's t-test (does not assume equal variances)\n",
        "roc_auc_t_test_result = stats.ttest_ind(gnn_roc_auc_scores, scibert_roc_auc_scores, equal_var=False)\n",
        "print(\"\\nROC-AUC Scores t-test Result:\", roc_auc_t_test_result)\n",
        "\n",
        "# Interpretation of Welch's t-test results\n",
        "print(\"Interpretation: There is\" + (\" a\" if roc_auc_t_test_result.pvalue < 0.05 else \" no\") + \" statistically significant difference in ROC-AUC scores between GNN and SciBERT models.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RxypViMS_UJ"
      },
      "source": [
        "## 7.II. Roc-Auc Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tArTXE5hSTxT"
      },
      "outputs": [],
      "source": [
        "# Load your data for hetroSciGNN and baseGNN models\n",
        "hetroSciGNN_cv_df = pd.read_pickle(hetroSciGNN_cv_results_path)\n",
        "baseGNN_cv_df = pd.read_pickle(baseGNN_cv_results_path)\n",
        "\n",
        "with open(SciBERT_valid_fold_metrics_path, 'rb') as file:\n",
        "    scibert_results = pickle.load(file)\n",
        "\n",
        "num_folds = 10\n",
        "# Function to compute average ROC curve for GNN models\n",
        "def compute_avg_roc_gnn(model_df):\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    mean_tpr = 0.0\n",
        "\n",
        "    for _, row in model_df.iterrows():\n",
        "        tpr_interpolated = np.interp(mean_fpr, row['fpr'], row['tpr'])\n",
        "        mean_tpr += tpr_interpolated\n",
        "\n",
        "    mean_tpr /= len(model_df)\n",
        "    mean_tpr[-1] = 1.0  # Ensure it ends at 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    return mean_fpr, mean_tpr, mean_auc\n",
        "\n",
        "# Compute average ROC curve for hetroSciGNN and baseGNN\n",
        "mean_fpr_hetroSciGNN, mean_tpr_hetroSciGNN, mean_auc_hetroSciGNN = compute_avg_roc_gnn(hetroSciGNN_cv_df)\n",
        "mean_fpr_baseGNN, mean_tpr_baseGNN, mean_auc_baseGNN = compute_avg_roc_gnn(baseGNN_cv_df)\n",
        "\n",
        "# SciBERT - Compute average ROC curve\n",
        "true_labels = scibert_results['true_labels']\n",
        "pred_probs = scibert_results['pred_probs']\n",
        "fold_size = len(true_labels) // (num_folds)\n",
        "mean_fpr_SciBERT = np.linspace(0, 1, 100)\n",
        "mean_tpr_SciBERT = 0.0\n",
        "\n",
        "for i in range(num_folds):\n",
        "    start_idx = i * fold_size\n",
        "    end_idx = (i + 1) * fold_size\n",
        "    fpr, tpr, _ = roc_curve(true_labels[start_idx:end_idx], pred_probs[start_idx:end_idx])\n",
        "    mean_tpr_SciBERT += np.interp(mean_fpr_SciBERT, fpr, tpr)\n",
        "\n",
        "mean_tpr_SciBERT /= num_folds\n",
        "mean_tpr_SciBERT[-1] = 1.0\n",
        "mean_auc_SciBERT = auc(mean_fpr_SciBERT, mean_tpr_SciBERT)\n",
        "\n",
        "# Plot the average ROC curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(mean_fpr_baseGNN, mean_tpr_baseGNN, color='green', lw=3,label=f'Baseline GNN (ROC-AUC = {mean_auc_baseGNN:.2f})')\n",
        "plt.plot(mean_fpr_SciBERT, mean_tpr_SciBERT, color='orange', lw=3, label=f'Fine-tuned SciBERT (ROC-AUC = {mean_auc_SciBERT:.2f})')\n",
        "plt.plot(mean_fpr_hetroSciGNN, mean_tpr_hetroSciGNN, color='blue', lw=3, label=f'DruGNNosis-MoA (ROC-AUC = {mean_auc_hetroSciGNN:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', lw=3, color='gray', label='Chance')\n",
        "plt.xlabel('False Positive Rate', fontsize = 16)\n",
        "plt.ylabel('True Positive Rate', fontsize = 16)\n",
        "plt.legend(fontsize = 16, loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.savefig('3_Models_Roc.pdf')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT1jJp3RSUuq"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(mean_fpr_SciBERT, mean_tpr_SciBERT,lw=3, color='orange', label=f'Fine-tuned SciBERT (ROC-AUC = {mean_auc_SciBERT:.4f})')\n",
        "plt.plot(mean_fpr_hetroSciGNN, mean_tpr_hetroSciGNN,lw=3, color='blue', label=f'DruGNNosis-MoA (ROC-AUC = {mean_auc_hetroSciGNN:.4f})')\n",
        "\n",
        "# Setting the axis limits to \"zoom in\" on the graph\n",
        "plt.xlim(0.0, 0.2)\n",
        "plt.ylim(0.4, 1.0)\n",
        "\n",
        "# plt.plot([0, 1], [0, 1], linestyle='--', lw=3, color='gray', label='Chance')\n",
        "\n",
        "plt.xlabel('False Positive Rate', fontsize = 16)\n",
        "plt.ylabel('True Positive Rate', fontsize = 16)\n",
        "plt.legend(loc='lower right', fontsize = 16)\n",
        "plt.grid(True)\n",
        "plt.savefig('LLM_GNN_ROC_AUC.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62CfFtrgTeBw"
      },
      "source": [
        "# 8. Generate Figures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih3iOR38sP7D"
      },
      "source": [
        "## 8.I. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6wNt9sk1NHb"
      },
      "outputs": [],
      "source": [
        "# Extract rows where Interaction Type is 'drug_target'\n",
        "data = df_all[df_all['Interaction Type'] == 'drug_target'].drop_duplicates(subset=['Source Name'])\n",
        "\n",
        "# Define a function for categorizing 'MoA'\n",
        "def categorize_moa(moa):\n",
        "    if 'e' in moa:\n",
        "        return 'Etiological'\n",
        "    elif 'p' in moa:\n",
        "        return 'Palliative'\n",
        "    else:\n",
        "        return 'Both'\n",
        "\n",
        "# Apply the function to the 'MoA' column\n",
        "data['MoA'] = data['MoA'].apply(categorize_moa)\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlgLCQhlv-ht"
      },
      "outputs": [],
      "source": [
        "categories = [ 'Palliative', 'Etiological', 'Both']\n",
        "\n",
        "# Define colors for each 'MoA'\n",
        "colors = [ 'steelblue', 'seagreen', 'goldenrod']\n",
        "\n",
        "# Count the occurrences of each 'MoA'\n",
        "moa_counts = data['MoA'].value_counts()\n",
        "print(moa_counts)\n",
        "# Calculate percentages for each 'MoA'\n",
        "moa_percentages = (moa_counts / moa_counts.sum()) * 100\n",
        "print(moa_percentages)\n",
        "\n",
        "# Create pie chart with specified colors\n",
        "fig, ax = plt.subplots()\n",
        "wedges, texts, autotexts = ax.pie(moa_percentages, colors=colors, autopct='%.3f%%', startangle=35)\n",
        "\n",
        "# Create the legend for the chart\n",
        "ax.legend(wedges, categories, title=\"Categories\", loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
        "\n",
        "# Save and show the plot\n",
        "plt.axis('off')\n",
        "plt.savefig('EvsP.pdf', format='pdf', dpi=1200, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK1ffpGG15FP"
      },
      "outputs": [],
      "source": [
        "# Define ATC classes as a dictionary\n",
        "atc_classes = {\n",
        "   \"A\": \"Alimentary tract and metabolism\",\n",
        "   \"B\": \"Blood and blood forming organs\",\n",
        "   \"C\": \"Cardiovascular system\",\n",
        "   \"D\": \"Dermatologicals\",\n",
        "   \"G\": \"Genito urinary system and sex hormones\",\n",
        "   \"H\": \"Systemic hormonal preparations, excl. sex hormones and insulins\",\n",
        "   \"J\": \"Antiinfectives for systemic use\",\n",
        "   \"L\": \"Antineoplastic and immunomodulating agents\",\n",
        "   \"M\": \"Musculo-skeletal system\",\n",
        "   \"N\": \"Nervous system\",\n",
        "   \"P\": \"Antiparasitic products, insecticides and repellents\",\n",
        "   \"R\": \"Respiratory system\",\n",
        "   \"S\": \"Sensory organs\",\n",
        "   \"V\": \"Various\",\n",
        "   \"Others\": \"Others\"\n",
        "}\n",
        "\n",
        "# Expand the 'ATC class' column into multiple rows\n",
        "atc_expanded = data['ATC Class'].str.split(',').apply(pd.Series, 1).stack()\n",
        "atc_expanded.index = atc_expanded.index.droplevel(-1)\n",
        "atc_expanded.name = 'ATC_Class'\n",
        "data_expanded = data.join(atc_expanded)\n",
        "\n",
        "# Ensure the ATC_Class column only has the letter codes\n",
        "data_expanded['ATC_Class'] = data_expanded['ATC_Class'].str.strip()\n",
        "\n",
        "# Group and count occurrences\n",
        "grouped_data = data_expanded.groupby(['ATC_Class', 'MoA']).size().unstack(fill_value=0)\n",
        "# Define colors for each 'MoA'\n",
        "moa_colors = {\n",
        "    'Both': 'goldenrod',\n",
        "    'Palliative': 'steelblue',\n",
        "    'Etiological': 'seagreen',\n",
        "\n",
        "}\n",
        "\n",
        "moa_order = ['Etiological', 'Palliative', 'Both']  # Define the order for coloring\n",
        "\n",
        "# Reorder the columns of grouped_data so that 'Both' is last (for stacking order)\n",
        "reordered_columns = [col for col in grouped_data if col != 'Both'] + ['Both']\n",
        "grouped_data = grouped_data[reordered_columns]\n",
        "\n",
        "# Plot the stacked bar chart\n",
        "ax = grouped_data.plot(kind='bar', stacked=True,  width=0.8, color=[moa_colors[moa] for moa in moa_order], figsize=(10, 7))\n",
        "ax.set_xlabel('ATC Class', fontsize=14)\n",
        "ax.set_ylabel('Frequency', fontsize=14)\n",
        "# ax.set_title('Frequency of Drugs by ATC Class and MOA') -> # For the paper don't need title for the figure.\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation='horizontal', fontsize=14) # Rotate x-axis labels to vertical\n",
        "\n",
        "# Create legend for 'MoA'\n",
        "moa_handles, moa_labels = ax.get_legend_handles_labels()\n",
        "moa_legend = ax.legend(moa_handles, moa_order, title='MoA', loc='upper left', fontsize=14)\n",
        "moa_legend.get_title().set_fontsize('large') # Set the font size for the 'MoA' legend title\n",
        "\n",
        "# Create custom legend for ATC classes without colors\n",
        "atc_legend_items = [Line2D([0], [0], marker='o', color='w', label=f'{key} - {desc}',\n",
        "                           markersize=0, markerfacecolor='none') for key, desc in atc_classes.items()]\n",
        "\n",
        "# Ensure that MoA legend remains visible\n",
        "ax.add_artist(moa_legend)\n",
        "\n",
        "# Save and show the plot\n",
        "plt.savefig('EPvsATC.pdf', format='pdf', dpi=1200, bbox_inches='tight')\n",
        "plt.show() # Display the chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkYY1FH10Fjr"
      },
      "source": [
        "## 8.II. Generate Heterogeneous Network Using NetworkX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XhwPAKMTh70"
      },
      "outputs": [],
      "source": [
        "# Initialize a NetworkX graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Create a mapping from drug name to MoA label\n",
        "drug_labels = drug_info_df.set_index('Source Name')['MoA'].to_dict()\n",
        "\n",
        "# Add nodes\n",
        "for drug in unique_drugs:\n",
        "    G.add_node(drug, node_type='drug', MoA=drug_labels.get(drug, 'unknown'))\n",
        "for disease in unique_diseases:\n",
        "    G.add_node(disease, node_type='disease')\n",
        "for gene in all_genes:\n",
        "    G.add_node(gene, node_type='gene')\n",
        "\n",
        "# Add edges for Drug-Gene interactions\n",
        "for src, tgt in zip(source_nodes_dg, target_nodes_dg):\n",
        "    G.add_edge(unique_drugs[src], all_genes[tgt], edge_type='drug_gene')\n",
        "\n",
        "# Add edges for Disease-Gene interactions\n",
        "for src, tgt in zip(source_nodes_pg, target_nodes_pg):\n",
        "    G.add_edge(unique_diseases[src], all_genes[tgt], edge_type='disease_gene')\n",
        "\n",
        "# Add edges for Gene-Gene interactions (PPI)\n",
        "for src, tgt in zip(source_nodes_gg, target_nodes_gg):\n",
        "    G.add_edge(all_genes[src], all_genes[tgt], edge_type='gene_gene')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXZSibpw1P_n"
      },
      "outputs": [],
      "source": [
        "# Now extract the different node sets\n",
        "drug_nodes_hetero = {node for node, data in G.nodes(data=True) if data.get('node_type') == 'drug'}\n",
        "disease_nodes_hetero = {node for node, data in G.nodes(data=True) if data.get('node_type') == 'disease'}\n",
        "gene_nodes_hetero = {node for node, data in G.nodes(data=True) if data.get('node_type') == 'gene'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNpyQyeyVKJu"
      },
      "outputs": [],
      "source": [
        "print(\"Graph Properties:\")\n",
        "print(f\"Total number of nodes: {G.number_of_nodes()}\")\n",
        "print(f\"Total number of edges: {G.number_of_edges()}\")\n",
        "\n",
        "# Node types: Drug, Disease, Gene\n",
        "print(f\"Number of Drug nodes: {sum(1 for _, attr in G.nodes(data=True) if attr['node_type'] == 'drug')}\")\n",
        "print(f\"Number of Disease nodes: {sum(1 for _, attr in G.nodes(data=True) if attr['node_type'] == 'disease')}\")\n",
        "print(f\"Number of Gene nodes: {sum(1 for _, attr in G.nodes(data=True) if attr['node_type'] == 'gene')}\")\n",
        "\n",
        "# Edge types: Drug-Gene, Disease-Gene, Gene-Gene\n",
        "print(f\"Number of Drug-Gene edges: {sum(1 for _, _, attr in G.edges(data=True) if attr['edge_type'] == 'drug_gene')}\")\n",
        "print(f\"Number of Disease-Gene edges: {sum(1 for _, _, attr in G.edges(data=True) if attr['edge_type'] == 'disease_gene')}\")\n",
        "print(f\"Number of Gene-Gene (PPI) edges: {sum(1 for _, _, attr in G.edges(data=True) if attr['edge_type'] == 'gene_gene')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE5XuVOm0Bb_"
      },
      "source": [
        "## 8.III. Heterogeneous Sub-Graph: Colorectal Cancer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3-KlGRYV4ju"
      },
      "outputs": [],
      "source": [
        "# Find the highest degree 'disease' node\n",
        "disease_nodes = [node for node in G.nodes if G.nodes[node]['node_type'] == 'disease']\n",
        "degree_dict = {node: G.degree(node) for node in disease_nodes}\n",
        "highest_degree_node = max(degree_dict, key=degree_dict.get)\n",
        "\n",
        "# Get all neighbors of the highest degree 'disease' node and its neighbors\n",
        "neighbors = list(G.neighbors(highest_degree_node))\n",
        "all_neighbors = set(neighbors)\n",
        "for neighbor in neighbors:\n",
        "    all_neighbors.update(G.neighbors(neighbor))\n",
        "\n",
        "# Create a new empty graph to store the subgraph\n",
        "H = nx.Graph()\n",
        "\n",
        "# Add 'gene' and 'drug' nodes to the subgraph H that are in all_neighbors\n",
        "for node_type in ['gene', 'drug']:\n",
        "    nodes_of_type = [node for node in all_neighbors if G.nodes[node]['node_type'] == node_type][:50]\n",
        "    for node in nodes_of_type:\n",
        "        H.add_node(node, **G.nodes[node])\n",
        "\n",
        "# Add the highest degree 'disease' node to the graph H along with their attributes\n",
        "H.add_node(highest_degree_node, **G.nodes[highest_degree_node])\n",
        "\n",
        "# Add the first-level neighbors of the highest degree 'disease' node to the graph H\n",
        "for neighbor in neighbors:\n",
        "    H.add_node(neighbor, **G.nodes[neighbor])\n",
        "\n",
        "# Add the edges between the nodes in H using the edges from the original graph G, excluding self-loops\n",
        "for edge in G.edges(data=True):\n",
        "    if edge[0] != edge[1] and edge[0] in H.nodes and edge[1] in H.nodes:\n",
        "        H.add_edge(edge[0], edge[1], **edge[2])\n",
        "\n",
        "# Compute the layout for visualization\n",
        "pos = nx.spring_layout(H, seed=123)\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Draw nodes with different colors and shapes based on their type\n",
        "node_colors = ['#1f77b4' if H.nodes[node]['node_type'] == 'gene' else\n",
        "               '#ff7f0e' if H.nodes[node]['node_type'] == 'drug' else\n",
        "               '#2ca02c' for node in H.nodes]\n",
        "node_shapes = ['o' if H.nodes[node]['node_type'] == 'gene' else\n",
        "               's' if H.nodes[node]['node_type'] == 'drug' else\n",
        "               '^' for node in H.nodes]\n",
        "\n",
        "# Draw the nodes\n",
        "for node_type, shape in [('gene', 'o'), ('drug', 's'), ('disease', '^')]:\n",
        "    nx.draw_networkx_nodes(H, pos, node_shape=shape,\n",
        "                           nodelist=[node for node in H.nodes if H.nodes[node]['node_type'] == node_type],\n",
        "                           node_color=[color for node, color in zip(H.nodes, node_colors) if H.nodes[node]['node_type'] == node_type],\n",
        "                           node_size=250)\n",
        "\n",
        "# Draw edges with different colors based on their type\n",
        "edge_colors = ['#d62728' if 'edge_type' in H.edges[edge] and H.edges[edge]['edge_type'] == 'drug_gene' else\n",
        "               '#9467bd' if 'edge_type' in H.edges[edge] and H.edges[edge]['edge_type'] == 'disease_gene' else\n",
        "               '#8c564b' for edge in H.edges]\n",
        "\n",
        "nx.draw_networkx_edges(H, pos, edge_color=edge_colors, width=0.5)\n",
        "\n",
        "# Add labels for drugs and diseases\n",
        "# Draw node labels for drugs with 'e' or 'p' instead of the drug name\n",
        "drug_labels = {node: ('e' if G.nodes[node]['MoA'] == 1 else 'p') for node in H.nodes() if G.nodes[node]['node_type'] == 'drug'}\n",
        "nx.draw_networkx_labels(H, pos, labels=drug_labels, font_size=14, font_color='black')\n",
        "\n",
        "# Draw label for the highest degree phenotype node with smaller font size\n",
        "label_pos = {highest_degree_node: (pos[highest_degree_node][0], pos[highest_degree_node][1]+0.60)}\n",
        "nx.draw_networkx_labels(H, label_pos, labels={highest_degree_node: highest_degree_node}, font_size=16, font_color='black')\n",
        "\n",
        "# Add a smaller arrow annotation pointing to the highest degree phenotype node and positioned a bit higher\n",
        "plt.annotate('', xy=pos[highest_degree_node], xytext=(pos[highest_degree_node][0], pos[highest_degree_node][1]+0.60),\n",
        "             arrowprops=dict(facecolor='black', shrink=0.05, width=0.5, headwidth=4))\n",
        "\n",
        "# Draw edges with specified colors based on their type\n",
        "edge_colors = []\n",
        "for u, v, data in H.edges(data=True):\n",
        "    etype = data.get('edge_type', 'gene_gene')\n",
        "    if etype == 'drug_gene':\n",
        "        edge_colors.append('black')\n",
        "    elif etype == 'disease_gene':\n",
        "        edge_colors.append('green')\n",
        "    else:\n",
        "        edge_colors.append('red')  # Gene-Gene edges are red\n",
        "nx.draw_networkx_edges(H, pos, edge_color=edge_colors, width=0.5)\n",
        "\n",
        "# Legends for nodes\n",
        "gene_patch = Line2D([], [], marker='o', color='w', label='Gene', markersize=14, markerfacecolor='#1f77b4')\n",
        "drug_patch = Line2D([], [], marker='s', color='w', label='Drug', markersize=14, markerfacecolor='#ff7f0e')\n",
        "disease_patch = Line2D([], [], marker='^', color='w', label='Disease', markersize=14, markerfacecolor='#2ca02c')\n",
        "node_legend = plt.legend(handles=[gene_patch, drug_patch, disease_patch], loc='lower left', fontsize='x-large')\n",
        "\n",
        "# Create a legend for the edge types\n",
        "edge_legend_items = [\n",
        "    Line2D([0], [0], color='black', lw=4, label='Drug-Target'),\n",
        "    Line2D([0], [0], color='green', lw=4, label='Disease-Gene'),\n",
        "    Line2D([0], [0], color='red', lw=4, label='Gene-Gene')\n",
        "]\n",
        "\n",
        "# Add both legends to the plot\n",
        "plt.gca().add_artist(node_legend)\n",
        "edge_legend = plt.legend(handles=edge_legend_items, loc='upper left', fontsize='x-large')\n",
        "\n",
        "# Save and show the plot\n",
        "plt.axis('off')\n",
        "plt.savefig('heterogeneous_networks.pdf', format='pdf', dpi=1200, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9knv3_grl93i"
      },
      "source": [
        "## 8.IV. Constructing Networks from the Datasets <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Topological Features For PPI & Drug-Target & Disease-Gene Networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF-Icij13nZl"
      },
      "source": [
        "#### 8.IV.A. Drug-Target Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnjRqBkH0JkT"
      },
      "outputs": [],
      "source": [
        "# Extract rows where Interaction Type is 'drug_target'\n",
        "drug_target_df = df_all[(df_all['Interaction Type'] == 'drug_target')]\n",
        "\n",
        "unique_drugs = drug_target_df['Source Name'].unique().tolist()\n",
        "drug_mapping = {drug: i for i, drug in enumerate(unique_drugs)}\n",
        "\n",
        "unique_genes_drug = drug_target_df['Target Name'].unique().tolist()\n",
        "genes_drug_mapping = {gene: i for i, gene in enumerate(unique_genes_drug)}\n",
        "\n",
        "# Drug-Gene interactions based on 'drug_target' interaction type\n",
        "source_nodes_dg = drug_target_df['Source Name'].map(drug_mapping).dropna().astype(int).values\n",
        "target_nodes_dg = drug_target_df['Target Name'].map(genes_drug_mapping).dropna().astype(int).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee_6DKAi0hDM"
      },
      "outputs": [],
      "source": [
        "# Initialize and build the NetworkX graph\n",
        "dt_net = nx.Graph()\n",
        "\n",
        "# Add drug nodes\n",
        "for drug in unique_drugs:\n",
        "    dt_net.add_node(drug, node_type='drug')\n",
        "\n",
        "# Add gene nodes\n",
        "for gene in unique_genes_drug:\n",
        "    dt_net.add_node(gene, node_type='gene')\n",
        "\n",
        "# Add edges for Drug-Gene interactions\n",
        "for src, tgt in zip(source_nodes_dg, target_nodes_dg):\n",
        "    dt_net.add_edge(unique_drugs[src], unique_genes_drug[tgt], edge_type='drug_gene')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZPfwLIa0tIp"
      },
      "outputs": [],
      "source": [
        "# Network statistics\n",
        "drug_nodes = {node for node, data in dt_net.nodes(data=True) if data.get('node_type') == 'drug'}\n",
        "gene_nodes = {node for node, data in dt_net.nodes(data=True) if data.get('node_type') == 'gene'}\n",
        "num_drug_nodes = len(drug_nodes)\n",
        "num_gene_nodes = len(gene_nodes)\n",
        "num_edges = dt_net.number_of_edges()\n",
        "\n",
        "# Connected components\n",
        "connected_components = list(nx.connected_components(dt_net))\n",
        "num_connected_components = len(connected_components)\n",
        "num_isolated_pairs = sum(1 for c in connected_components if len(c) == 2)\n",
        "\n",
        "# Largest Connected Component (LCC)\n",
        "largest_cc = max(connected_components, key=len)\n",
        "LCC_dt = dt_net.subgraph(largest_cc)\n",
        "\n",
        "# Counting drug and gene nodes in LCC\n",
        "drug_nodes_LCC = {node for node, data in LCC_dt.nodes(data=True) if data.get('node_type') == 'drug'}\n",
        "gene_nodes_LCC = {node for node, data in LCC_dt.nodes(data=True) if data.get('node_type') == 'gene'}\n",
        "num_drug_nodes_LCC = len(drug_nodes_LCC)\n",
        "num_gene_nodes_LCC = len(gene_nodes_LCC)\n",
        "num_edges_LCC = LCC_dt.number_of_edges()\n",
        "num_nodes_LCC = LCC_dt.number_of_nodes()\n",
        "\n",
        "# Degree analysis in LCC\n",
        "degree_sequence_LCC = sorted((d for n, d in LCC_dt.degree()), reverse=True)\n",
        "degree_count_LCC = Counter(degree_sequence_LCC)\n",
        "most_frequent_degree_LCC = degree_count_LCC.most_common(1)[0][0]\n",
        "\n",
        "# Output\n",
        "print(f\"Network Statistics:\")\n",
        "print(f\"- Number of drug nodes in the entire network: {num_drug_nodes}\")\n",
        "print(f\"- Number of gene target nodes in the entire network: {num_gene_nodes}\")\n",
        "print(f\"- Number of links (edges) connecting them: {num_edges}\")\n",
        "print(f\"- Number of connected components: {num_connected_components}\")\n",
        "print(f\"- Number of isolated pairs (standalone drug-target pairs): {num_isolated_pairs}\")\n",
        "print(f\"\\nLargest Connected Component (LCC):\")\n",
        "print(f\"- Total number of nodes in LCC: {num_nodes_LCC}\")\n",
        "print(f\"- Number of drug nodes in LCC: {num_drug_nodes_LCC}\")\n",
        "print(f\"- Number of gene target nodes in LCC: {num_gene_nodes_LCC}\")\n",
        "print(f\"- Number of links (edges) in LCC: {num_edges_LCC}\")\n",
        "print(f\"- Most frequent degree in LCC: {most_frequent_degree_LCC}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZV00TvpPxWg"
      },
      "source": [
        "##### Generate Figure For The Largest Connected Component of the Drug-Target Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s_xRlb3PLR8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "pos = nx.spring_layout(LCC_dt, seed=24) # Using spring layout for positioning nodes\n",
        "nx.draw(LCC_dt, pos, with_labels=False, node_size=20, edge_color=\"gray\", linewidths=0.4)\n",
        "plt.tight_layout()\n",
        "plt.title(\"Largest Connected Component of the Drug-Target Network\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaDdi5wZpcug"
      },
      "source": [
        "#### 8.IV.B. Protein-Protein Interaction (PPI) Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FfwT3Aa8WWj"
      },
      "outputs": [],
      "source": [
        "# Extract unique genes based on Interaction Type being 'PPI'\n",
        "PPI_df = df_all[df_all['Interaction Type'] == 'PPI']\n",
        "\n",
        "# Get unique genes from both 'Source Name' and 'Target Name'\n",
        "unique_genes_ppi = set(PPI_df['Source Name'].unique()).union(set(PPI_df['Target Name'].unique()))\n",
        "unique_genes_ppi = list(unique_genes_ppi)  # Convert set back to list\n",
        "PPI_genes_mapping = {gene: i for i, gene in enumerate(unique_genes_ppi)}\n",
        "\n",
        "# Gene-Gene interactions (PPI)\n",
        "source_nodes_gg = PPI_df['Source Name'].map(PPI_genes_mapping).dropna().astype(int).values\n",
        "target_nodes_gg = PPI_df['Target Name'].map(PPI_genes_mapping).dropna().astype(int).values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIUn2QI8pub2"
      },
      "source": [
        "Generate PPI Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKMzpMaXmzxN"
      },
      "outputs": [],
      "source": [
        "# Initialize a NetworkX graph\n",
        "PPI = nx.Graph()\n",
        "\n",
        "# Add gene nodes\n",
        "for gene in unique_genes_ppi:\n",
        "    PPI.add_node(gene, node_type='gene')\n",
        "\n",
        "# Add edges for Gene-Gene interactions (PPI)\n",
        "for src, tgt in zip(source_nodes_gg, target_nodes_gg):\n",
        "    PPI.add_edge(unique_genes_ppi[src], unique_genes_ppi[tgt], edge_type='gene_gene')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOFA6iNFpxsG"
      },
      "source": [
        "PPI Topological Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT2G7TmIpznK"
      },
      "outputs": [],
      "source": [
        "# Calculate basic network statistics\n",
        "num_ppi_nodes = PPI.number_of_nodes()\n",
        "num_ppi_edges = PPI.number_of_edges()\n",
        "num_ppi_self_loops = nx.number_of_selfloops(PPI)\n",
        "\n",
        "# Remove self-loops\n",
        "PPI.remove_edges_from(nx.selfloop_edges(PPI))\n",
        "\n",
        "# Identify connected components and calculate statistics\n",
        "connected_components = list(nx.connected_components(PPI))\n",
        "num_connected_components = len(connected_components)\n",
        "num_isolated_nodes = sum(1 for c in connected_components if len(c) == 1)\n",
        "\n",
        "# Determine the Largest Connected Component (LCC)\n",
        "largest_cc = max(connected_components, key=len)\n",
        "LCC_PPI = PPI.subgraph(largest_cc)\n",
        "num_nodes_LCC = len(largest_cc)\n",
        "num_edges_LCC = LCC_PPI.number_of_edges()\n",
        "\n",
        "# Counting gene nodes in the LCC\n",
        "gene_nodes_LCC = {node for node, data in LCC_PPI.nodes(data=True) if data.get('node_type') == 'gene'}\n",
        "num_gene_nodes_LCC = len(gene_nodes_LCC)\n",
        "\n",
        "# Degree analysis in the LCC\n",
        "degree_sequence = sorted((d for n, d in LCC_PPI.degree()), reverse=True)\n",
        "degree_count = Counter(degree_sequence)\n",
        "most_frequent_degree = degree_count.most_common(1)[0][0]\n",
        "\n",
        "# Calculate the average clustering coefficient in the LCC\n",
        "avg_clustering_coef = nx.average_clustering(LCC_PPI)\n",
        "\n",
        "# Output the statistics\n",
        "print(f\"PPI network:\")\n",
        "print(f\"- Total number of protein nodes: {num_ppi_nodes}\")\n",
        "print(f\"- Total number of edges (excluding self-loops): {num_ppi_edges - num_ppi_self_loops}\")\n",
        "print(f\"- Number of self-loop edges: {num_ppi_self_loops}\")\n",
        "print(f\"- Number of connected components: {num_connected_components}\")\n",
        "print(f\"- Number of isolated protein nodes: {num_isolated_nodes}\")\n",
        "print(f\"\\nLargest Connected Component (LCC):\")\n",
        "print(f\"- Total number of nodes in LCC: {num_nodes_LCC}\")\n",
        "print(f\"- Number of gene nodes in LCC: {num_gene_nodes_LCC}\")\n",
        "print(f\"- Number of edges in LCC: {num_edges_LCC}\")\n",
        "print(f\"- Most frequent degree in LCC: {most_frequent_degree}\")\n",
        "print(f\"- Average clustering coefficient in LCC: {avg_clustering_coef}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMv_u6QkPuCY"
      },
      "source": [
        "##### Generate Figure For The Largest Connected Component of the PPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPoNYNdSPDp0"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(10, 8))\n",
        "# pos = nx.spring_layout(LCC_PPI, seed=24)  # Using spring layout for positioning nodes\n",
        "# nx.draw(LCC_PPI, pos, with_labels=False, node_size=20, edge_color=\"gray\", linewidths=0.4)\n",
        "# plt.tight_layout()\n",
        "# plt.title(\"Largest Connected Component of the PPI Network\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfKknZWEAWdR"
      },
      "source": [
        "### 8.IV.C. Disease-Gene Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W92Hs6sKWIII"
      },
      "outputs": [],
      "source": [
        "# Extract unique genes based on Interaction Type being 'association'\n",
        "disease_gene_df = df_all[df_all[\"Interaction Type\"] == \"association\"]\n",
        "\n",
        "unique_disease = disease_gene_df['Source Name'].unique().tolist()\n",
        "disease_mapping = {disease: i for i, disease in enumerate(unique_disease)}\n",
        "\n",
        "unique_gene_association = disease_gene_df['Target Name'].unique().tolist()\n",
        "gene_association_mapping = {gene: i for i, gene in enumerate(unique_gene_association)}\n",
        "\n",
        "# Disease-Gene interactions\n",
        "source_nodes_pg = disease_gene_df['Source Name'].map(disease_mapping).dropna().astype(int).values\n",
        "target_nodes_pg = disease_gene_df['Target Name'].map(gene_association_mapping).dropna().astype(int).values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMaIzZJELh3M"
      },
      "source": [
        "Generate Disease-Gene (Diseasome) network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0vx8aSIKTyp"
      },
      "outputs": [],
      "source": [
        "# Initialize and build the NetworkX graph\n",
        "diseasome_net = nx.Graph()\n",
        "\n",
        "# Add disease nodes\n",
        "for disease in unique_disease:\n",
        "    diseasome_net.add_node(disease, node_type='disease')\n",
        "\n",
        "# Add gene nodes\n",
        "for gene in unique_gene_association:\n",
        "    diseasome_net.add_node(gene, node_type='gene')\n",
        "\n",
        "# Add edges for Disease-Gene interactions\n",
        "for src, tgt in zip(source_nodes_pg, target_nodes_pg):\n",
        "    diseasome_net.add_edge(unique_disease[src], unique_gene_association[tgt], edge_type='disease_gene')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjdXOOJqPe10"
      },
      "source": [
        " Disease-Gene Topological Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJfWUY1wKUEX"
      },
      "outputs": [],
      "source": [
        "# Calculate basic network statistics\n",
        "disease_nodes = {node for node, data in diseasome_net.nodes(data=True) if data.get('node_type') == 'disease'}\n",
        "gene_nodes = {node for node, data in diseasome_net.nodes(data=True) if data.get('node_type') == 'gene'}\n",
        "num_disease_nodes = len(disease_nodes)\n",
        "num_gene_nodes = len(gene_nodes)\n",
        "num_edges = diseasome_net.number_of_edges()\n",
        "\n",
        "# Identify connected components and calculate statistics\n",
        "connected_components = list(nx.connected_components(diseasome_net))\n",
        "num_connected_components = len(connected_components)\n",
        "num_isolated_pairs = sum(1 for c in connected_components if len(c) == 2)\n",
        "\n",
        "# Determine the Largest Connected Component (LCC)\n",
        "largest_cc = max(connected_components, key=len)\n",
        "LCC_diseasome = diseasome_net.subgraph(largest_cc)\n",
        "num_nodes_LCC = len(largest_cc)\n",
        "num_edges_LCC = LCC_diseasome.number_of_edges()\n",
        "\n",
        "# Counting disease and gene nodes in the LCC\n",
        "disease_nodes_LCC = {node for node, data in LCC_diseasome.nodes(data=True) if data.get('node_type') == 'disease'}\n",
        "gene_nodes_LCC = {node for node, data in LCC_diseasome.nodes(data=True) if data.get('node_type') == 'gene'}\n",
        "num_disease_nodes_LCC = len(disease_nodes_LCC)\n",
        "num_gene_nodes_LCC = len(gene_nodes_LCC)\n",
        "\n",
        "# Degree analysis in the LCC\n",
        "degree_sequence_LCC = sorted((d for n, d in LCC_diseasome.degree()), reverse=True)\n",
        "degree_count_LCC = Counter(degree_sequence_LCC)\n",
        "most_frequent_degree_LCC = degree_count_LCC.most_common(1)[0][0]\n",
        "\n",
        "# Output the statistics\n",
        "print(f\"Diseasome Network Statistics:\")\n",
        "print(f\"- Number of disease nodes in the entire network: {num_disease_nodes}\")\n",
        "print(f\"- Number of gene nodes in the entire network: {num_gene_nodes}\")\n",
        "print(f\"- Number of links (edges) connecting them: {num_edges}\")\n",
        "print(f\"- Number of connected components: {num_connected_components}\")\n",
        "print(f\"- Number of isolated pairs (standalone disease-gene pairs): {num_isolated_pairs}\")\n",
        "print(f\"\\nLargest Connected Component (LCC):\")\n",
        "print(f\"- Total number of nodes in LCC: {num_nodes_LCC}\")\n",
        "print(f\"- Number of disease nodes in LCC: {num_disease_nodes_LCC}\")\n",
        "print(f\"- Number of gene nodes in LCC: {num_gene_nodes_LCC}\")\n",
        "print(f\"- Number of links (edges) in LCC: {num_edges_LCC}\")\n",
        "print(f\"- Most frequent degree in LCC: {most_frequent_degree_LCC}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bvG-uBLPlqf"
      },
      "source": [
        "#### Generate Figure For The Largest Connected Component of the Diseasome Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsKgIbWXOcbV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "pos = nx.spring_layout(LCC_diseasome, seed=24)\n",
        "nx.draw(LCC_diseasome, pos, with_labels=False, node_size=20, edge_color=\"gray\", linewidths=0.4)\n",
        "plt.tight_layout()\n",
        "plt.title(\"Largest Connected Component of the Diseasome Network\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8QPQsLG2pem"
      },
      "source": [
        "# 9. Realted work - Yildrim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gO1Z1nRwwAqC"
      },
      "outputs": [],
      "source": [
        "# Calculate the shortest paths and store in a list\n",
        "shortest_paths = []\n",
        "\n",
        "for drug in tqdm(drug_nodes_hetero, desc='Calculate path'):\n",
        "    drug_moa = G.nodes[drug].get('MoA', 'unknown')  # Retrieve MoA of the drug\n",
        "    for disease in disease_nodes_hetero:\n",
        "        try:\n",
        "            path = nx.shortest_path(G, source=drug, target=disease)\n",
        "            intermediate_nodes = set(path[1:-1])\n",
        "            if intermediate_nodes.issubset(gene_nodes_hetero):\n",
        "                \"\"\"\n",
        "                -2 to remove drug and disease from the path length.\n",
        "                -1 to calculate the distance by the edges.\n",
        "                for example: path = [DrugA, GeneA, GeneB, GeneC, disease], so the path length is 5.\n",
        "                But the distance will be 2 because we don't count the drug and disease and calculate the number of edges between GeneA to Gene C.\n",
        "                \"\"\"\n",
        "                path_length = len(path) - 3  # Calculating the path length\n",
        "                shortest_paths.append((drug, drug_moa, disease, path_length, path))\n",
        "        except nx.NetworkXNoPath:\n",
        "            # Record the absence of a path\n",
        "            shortest_paths.append((drug, drug_moa, None, None, None))\n",
        "\n",
        "# Filter the shortest paths for each drug\n",
        "all_shortest_paths = defaultdict(list)\n",
        "for drug, drug_moa, disease, length, path in shortest_paths:\n",
        "    if disease is not None:  # If there is a path\n",
        "        all_shortest_paths[drug].append((drug_moa, disease, length, path))\n",
        "\n",
        "# Find the shortest path for each drug\n",
        "unique_shortest_paths = []\n",
        "for drug, paths in all_shortest_paths.items():\n",
        "    min_length = min(length for _, _, length, _ in paths)\n",
        "    for drug_moa, disease, length, path in paths:\n",
        "        if length == min_length:\n",
        "            unique_shortest_paths.append((drug, drug_moa, disease, length, path))\n",
        "\n",
        "# Flatten all_shortest_paths into a list of tuples\n",
        "arr_all_shortest_paths = []\n",
        "for drug, paths in all_shortest_paths.items():\n",
        "    for drug_moa, disease, length, path in paths:\n",
        "        arr_all_shortest_paths.append((drug, drug_moa, disease, length, path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ4fqLgjtoGO"
      },
      "source": [
        "Drugs with no path to disease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuFvCKC8VuHx"
      },
      "outputs": [],
      "source": [
        "drugs_without_paths = drug_nodes_hetero - {drug for drug, _, _, _, _ in shortest_paths if _ is not None}\n",
        "print(drugs_without_paths)\n",
        "print(len(drugs_without_paths))\n",
        "\n",
        "for drug in drugs_without_paths:\n",
        "    drug_moa = G.nodes[drug].get('MoA', 'unknown')  # Retrieve MoA of the drug\n",
        "    print(f\"{drug}, {drug_moa}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyosXFNOWa1_"
      },
      "source": [
        "Save All vs. All dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bchVuJv2V8f1"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame\n",
        "df_columns = ['Drug', 'MoA', 'Disease', 'Path Length', 'Path']\n",
        "df_all = pd.DataFrame(arr_all_shortest_paths, columns=df_columns)\n",
        "\n",
        "# Save the DataFrame to a compressed csv.gz format\n",
        "df_all.to_csv('All_shortest_paths_compress.csv.gz', compression='gzip', index=False)\n",
        "df_all.to_csv('All_shortest_paths.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6qGSCniWbcL"
      },
      "source": [
        "Save Unique Shortest Distance dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTGYGcsWV8wH"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame\n",
        "df_columns = ['Drug', 'MoA', 'Disease', 'Path Length', 'Path']\n",
        "df_unique = pd.DataFrame(unique_shortest_paths, columns=df_columns)\n",
        "\n",
        "# Save the DataFrame to a compressed csv.gz format\n",
        "df_unique.to_csv('Unique_shortest_paths_compress.csv.gz', compression='gzip', index=False)\n",
        "df_unique.to_csv('Unique_shortest_paths.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFYcAsAhbKIp"
      },
      "source": [
        "### 1. **All vs. All Method:** Calculates and counts the shortest distance from each drug to every disease, considering each occurrence of the drug in multiple paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVGkfNwabZQY"
      },
      "outputs": [],
      "source": [
        "df_all_shortest_paths = pd.read_csv('All_shortest_paths_compress.csv.gz', compression='gzip', low_memory=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1hs4tCZCZpP"
      },
      "outputs": [],
      "source": [
        "print(df_all_shortest_paths.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m_6hvafI6A6"
      },
      "outputs": [],
      "source": [
        "# Count occurrence for each path length\n",
        "path_counts = df_all_shortest_paths['Path Length'].value_counts()\n",
        "\n",
        "# Calculate the fraction for each path length\n",
        "total_paths = len(df_all_shortest_paths)\n",
        "fractions = path_counts / total_paths\n",
        "fractions = fractions.sort_index()\n",
        "\n",
        "# Initialize the dictionaries\n",
        "etiological_counts = defaultdict(int)\n",
        "palliative_counts = defaultdict(int)\n",
        "\n",
        "# Calculate the counts for each path length and MoA category\n",
        "moa_counts = df_all_shortest_paths.groupby(['Path Length', 'MoA']).size().unstack(fill_value=0)\n",
        "print(moa_counts)\n",
        "\n",
        "# Since 'MoA' contains 0 for etiological and 1 for palliative\n",
        "etiological_counts = moa_counts[0].to_dict()\n",
        "palliative_counts = moa_counts[1].to_dict()\n",
        "\n",
        "# Print to check if the dictionaries are now correctly populated\n",
        "print(etiological_counts)\n",
        "print(palliative_counts)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fractions.index, fractions.values, marker='^', label='Disease genes versus drug targets', color='black', linestyle='-')\n",
        "\n",
        "# Annotating each point with etiological and palliative\n",
        "for distance in fractions.index:\n",
        "    etiological_count = etiological_counts.get(distance, 0)\n",
        "    palliative_count = palliative_counts.get(distance, 0)\n",
        "\n",
        "    # Adjust the y_offset to your preference for text placement\n",
        "    y_offset = 0.0009\n",
        "\n",
        "    plt.annotate(f'e:{etiological_count}', (distance, fractions[distance] + y_offset), textcoords=\"offset points\", xytext=(0,6), ha='center', fontsize=15)\n",
        "    plt.annotate(f'p:{palliative_count}', (distance, fractions[distance] - y_offset), textcoords=\"offset points\", xytext=(0,-15), ha='center', fontsize=15)\n",
        "\n",
        "plt.xlabel('Drug-Disease Distance', fontsize = 16)\n",
        "plt.ylabel('Fraction of Total Distances', fontsize = 16)\n",
        "#plt.title('Distribution of Shortest Distance between Drug Target and Disease Gene')\n",
        "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig('Yildrim_all_shortest_distance.pdf', format='pdf', dpi=1200, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ViTD-V8dxn9"
      },
      "source": [
        "### 2. **All vs. All (Unique) Method:** Computes the shortest distance from each drug to every disease, counting each drug only once for each unique path length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrVp7h2AaZFy"
      },
      "outputs": [],
      "source": [
        "# Drop duplicates to ensure that each drug is only counted once per distance\n",
        "df_unique_per_distance = df_all_shortest_paths.drop_duplicates(subset=['Drug', 'Path Length'])\n",
        "\n",
        "# Count unique occurrences for each path length\n",
        "unique_path_counts = df_unique_per_distance['Path Length'].value_counts()\n",
        "\n",
        "# Calculate the fraction for each path length based on unique drug counts\n",
        "total_unique_drugs = df_unique_per_distance['Drug'].nunique()\n",
        "unique_fractions = unique_path_counts / total_unique_drugs\n",
        "unique_fractions = unique_fractions.sort_index()\n",
        "\n",
        "# Now use unique_fractions for plotting\n",
        "# Initialize the dictionaries\n",
        "etiological_counts = defaultdict(int)\n",
        "palliative_counts = defaultdict(int)\n",
        "\n",
        "# Calculate the counts for each path length and MoA category\n",
        "moa_counts = df_unique_per_distance.groupby(['Path Length', 'MoA']).size().unstack(fill_value=0)\n",
        "\n",
        "# Since 'MoA' contains 0 for etiological and 1 for palliative\n",
        "etiological_counts = moa_counts[0].to_dict()\n",
        "palliative_counts = moa_counts[1].to_dict()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fractions.index, unique_fractions.values, marker='^', label='Disease genes versus drug targets', color='black', linestyle='-')\n",
        "\n",
        "# Annotating each point with etiological and palliative\n",
        "for distance in unique_fractions.index:\n",
        "    etiological_count = etiological_counts.get(distance, 0)\n",
        "    palliative_count = palliative_counts.get(distance, 0)\n",
        "\n",
        "    # Adjust the y_offset to your preference for text placement\n",
        "    y_offset = 0.0001\n",
        "\n",
        "    plt.annotate(f'e:{etiological_count}', (distance, unique_fractions[distance] + y_offset), textcoords=\"offset points\", xytext=(0,6), ha='center', fontsize=15)\n",
        "    plt.annotate(f'p:{palliative_count}', (distance, unique_fractions[distance] - y_offset), textcoords=\"offset points\", xytext=(0,-15), ha='center', fontsize=15)\n",
        "\n",
        "plt.xlabel('Drug-Disease Distance', fontsize = 16)\n",
        "plt.ylabel('Fraction of Total Distances', fontsize = 16)\n",
        "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig('Yildrim_ALL_unique_drug_count_shortest_distance.pdf', format='pdf', dpi=1200, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8lTowvsbMXR"
      },
      "source": [
        "### 3. **Unique Minimum Shortest Distance Method:** For each drug, we determined its shortest distance to a disease. If one drug had the same shortest distance for several diseases, we eliminated the duplicate values, retaining only one unique shortest distance for each drug."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV-70ocMEEeG"
      },
      "outputs": [],
      "source": [
        "df_unique_shortest_paths = pd.read_csv('Unique_shortest_paths_compress.csv.gz', compression='gzip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tfppd3yfM0fR"
      },
      "outputs": [],
      "source": [
        "df_unique_shortest_paths = df_unique_shortest_paths.drop_duplicates(subset=['Drug', 'Path Length'])\n",
        "\n",
        "# Count occurrence for each path length\n",
        "path_counts = df_unique_shortest_paths['Path Length'].value_counts()\n",
        "\n",
        "# Calculate the fraction for each path length\n",
        "total_paths = len(df_unique_shortest_paths)\n",
        "fractions = path_counts / total_paths\n",
        "fractions = fractions.sort_index()\n",
        "\n",
        "# Initialize the dictionaries\n",
        "etiological_counts = defaultdict(int)\n",
        "palliative_counts = defaultdict(int)\n",
        "\n",
        "# Calculate the counts for each path length and MoA category\n",
        "moa_counts = df_unique_shortest_paths.groupby(['Path Length', 'MoA']).size().unstack(fill_value=0)\n",
        "\n",
        "# Since 'MoA' contains 0 for etiological and 1 for palliative\n",
        "etiological_counts = moa_counts[0].to_dict()\n",
        "palliative_counts = moa_counts[1].to_dict()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fractions.index, fractions.values, marker='^', label='Disease genes versus drug targets', color='black', linestyle='-')\n",
        "\n",
        "# Annotating each point with etiological, palliative, and baseline counts\n",
        "for distance in fractions.index:\n",
        "    etiological_count = etiological_counts.get(distance, 0)\n",
        "    palliative_count = palliative_counts.get(distance, 0)\n",
        "\n",
        "    # Adjust the y_offset to your preference for text placement\n",
        "    y_offset = 0.0001\n",
        "\n",
        "    plt.annotate(f'e:{etiological_count}', (distance, fractions[distance] + y_offset), textcoords=\"offset points\", xytext=(0,6), ha='center', fontsize=15)\n",
        "    plt.annotate(f'p:{palliative_count}', (distance, fractions[distance] - y_offset), textcoords=\"offset points\", xytext=(0,-15), ha='center', fontsize=15)\n",
        "\n",
        "plt.xlabel('Drug-Disease Distance', fontsize = 16)\n",
        "plt.ylabel('Fraction of Total Distances', fontsize = 16)\n",
        "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig('Yildrim_unique_minimum_shortest_distance.pdf', format='pdf', dpi=1200, bbox_inches='tight')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "m6_RUEQ3BAAi",
        "zXkwliNcAf0m",
        "Ni_uUYbi_MkF",
        "tZV00TvpPxWg"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}